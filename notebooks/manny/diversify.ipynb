{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex.li/miniconda3/envs/cvml/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from brtdevkit.data import Dataset\n",
    "from email.mime import image\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset as torchDataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path \n",
    "from sklearn.decomposition import PCA\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# types: for 4-class, 1 is human, 2 is vehicle\n",
    "def n_pixels_in_grid(details, types = [1]):\n",
    "    total_size = 0\n",
    "    for k, v in ast.literal_eval(details).items():\n",
    "        if k in types:\n",
    "            total_size += sum(vi['object_size'] for vi in v)\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups = df.groupby('group_id')\n",
    "# all_sizes = []\n",
    "# ok_ids = []\n",
    "# for group_id, group_df in groups:\n",
    "#     best_ind = 0\n",
    "#     best = n_pixels_in_grid(df['label_grid_details'].iloc[best_ind])\n",
    "#     for i in range(1, len(group_df)):\n",
    "#         cur = n_pixels_in_grid(details=group_df['label_grid_details'].iloc[i])\n",
    "#         if cur > best:\n",
    "#             best = cur\n",
    "#     ok_ids.append(group_df.iloc[best_ind]['id'])\n",
    "#     all_sizes.append(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasetpreparer(torchDataset):    \n",
    "        def __init__(self, data, preprocess=None, transform=None, device=\"cuda\"):\n",
    "            self.data = data\n",
    "            self.transform =  transform \n",
    "            self.preprocess = preprocess\n",
    "            self.device = device \n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_path = self.data.loc[idx, 'image_path']\n",
    "            img_pil = self.transform(Image.fromarray(imageio.imread(img_path)))\n",
    "            # Download and open the image        \n",
    "            img_pil = self.preprocess(img_pil).to(self.device)\n",
    "            return img_pil, img_path\n",
    "\n",
    "class ImageSimilarity: \n",
    "    def __init__(self, images_full_path, data_base_path, dataset_name='image_list', overwrite=False):\n",
    "        self.data_base_path = data_base_path\n",
    "        os.makedirs(self.data_base_path, exist_ok=True)\n",
    "        self.images_base_path = data_base_path + \"/images\"\n",
    "        self.images_full_path = images_full_path\n",
    "        \n",
    "        self.image_path_df = None \n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((224,224))])\n",
    "        \n",
    "        self.inference_set = None \n",
    "        self.embeddings_np = None\n",
    "        self.image_paths = None \n",
    "        self.image_paths_df = None\n",
    "        self.embeddings_save_name = f\"{dataset_name}_embeddings\"\n",
    "        self.embeddings_image_paths = Path(self.data_base_path) / f\"{dataset_name}_embeddings_image_paths.csv\"\n",
    "        self.embeddings_save_loc = Path(self.data_base_path) / self.embeddings_save_name\n",
    "        self.overwrite = overwrite \n",
    "        self.model = None \n",
    "        self.preprocessor = None \n",
    "        self.sorted_scores = None \n",
    "        self.sorted_scores_save_loc = data_base_path \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.load_model()\n",
    "        \n",
    "    def prepare_images_path_df(self): \n",
    "        self.image_path_df = pd.DataFrame(data=self.images_full_path, columns=['image_path'])\n",
    "        return self.image_path_df\n",
    "    \n",
    "    def prepare_dataloader(self): \n",
    "        self.inference_set = Datasetpreparer(data=self.image_path_df, preprocess=self.preprocessor, transform=self.transform, device=self.device)\n",
    "        inference_loader = DataLoader(self.inference_set, batch_size=64, shuffle=False, num_workers=0, drop_last=True)\n",
    "        self.total = len(self.image_path_df) / 64\n",
    "        return inference_loader\n",
    " \n",
    "    def load_model(self): \n",
    "        self.model, self.preprocessor = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        return self.model, self.preprocessor\n",
    "\n",
    "    def build_embeddings(self): \n",
    "        outputs = []\n",
    "        image_paths = []\n",
    "        if (os.path.isfile(self.embeddings_save_loc)==True) or (self.overwrite==False): \n",
    "            self.embeddings_np, self.image_paths = self.load_embeddings()\n",
    "            return self.embeddings_np, self.image_paths \n",
    "        else:\n",
    "            inference_loader = self.prepare_dataloader()\n",
    "            for idx, batch in tqdm(enumerate(inference_loader), total=self.total): \n",
    "                batch, paths = batch[0], batch[1]\n",
    "                with torch.no_grad():\n",
    "                        batch = batch.to(self.device)\n",
    "                        image_features = self.model.encode_image(batch)\n",
    "                        outputs.append(image_features)  \n",
    "                        image_paths.extend(paths)\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "            self.image_paths = image_paths \n",
    "            self.embeddings_np = outputs.detach().cpu().numpy()\n",
    "            return self.embeddings_np, self.image_paths\n",
    "            \n",
    "    def save_embeddings(self): \n",
    "            np.savez(self.embeddings_save_loc, self.embeddings_np) \n",
    "            image_paths_df = pd.DataFrame(data=self.image_paths, columns=['image_path'])\n",
    "            image_paths_df.to_csv(self.embeddings_image_paths, index=False)\n",
    "            return None \n",
    "        \n",
    "    def load_embeddings(self): \n",
    "        embeddings_save_loc = str(self.embeddings_save_loc) +\".npz\"\n",
    "        embeddings = np.load(embeddings_save_loc)\n",
    "        self.embeddings_np = embeddings[embeddings.files[0]]       \n",
    "        self.image_paths_df= pd.read_csv(self.embeddings_image_paths )\n",
    "        return self.embeddings_np, self.image_paths_df \n",
    "\n",
    "    def get_image_embedding(self, image_path): \n",
    "        transformed_image = self.transform(Image.open(image_path))\n",
    "        img = self.preprocessor(transformed_image).unsqueeze(0).to(self.device)\n",
    "        embedding = self.model.encode_image(img)\n",
    "        embeddings_np = embedding.detach().cpu().numpy()\n",
    "        return embeddings_np\n",
    "    \n",
    "    def get_similar_images(self, image_path, calcuate_embeddings=False, images_base_path=None): \n",
    "        if images_base_path==None:\n",
    "            images_base_path = self.images_base_path\n",
    "        full_image_path = os.path.join(images_base_path, image_path)\n",
    "        embeddings_np, image_paths_df = self.load_embeddings()\n",
    "        image_paths = image_paths_df['image_path'].tolist()\n",
    "        if calcuate_embeddings==True:\n",
    "            reference_embedding = self.get_image_embedding(full_image_path)\n",
    "        else :\n",
    "            embedding_index = image_paths_df[image_paths_df['image_path'] == full_image_path].index[0]\n",
    "            reference_embedding = embeddings_np[embedding_index, :]\n",
    "        \n",
    "        similarity_score = np.dot(reference_embedding, embeddings_np.T)/(np.linalg.norm(reference_embedding)*np.linalg.norm( embeddings_np, axis=1))\n",
    "        score_df = pd.DataFrame(data=list(similarity_score.T), columns=['similarity_score'])\n",
    "        score_df['image_path'] = image_paths \n",
    "        self.sorted_scores = score_df.sort_values(by='similarity_score', ascending=False)\n",
    "        image_name = image_path.split(\"/\")[-1].replace(\".png\",\"\")\n",
    "        save_loc_sorted_scores = Path(self.sorted_scores_save_loc) / f\"{image_name}.csv\"\n",
    "        self.sorted_scores.to_csv(save_loc_sorted_scores, index=False)\n",
    "        return self.sorted_scores\n",
    "    \n",
    "    def similar_images_with_text(self, text_prompt):\n",
    "        \n",
    "        embeddings_np, image_paths_df = self.load_embeddings()\n",
    "        image_paths = image_paths_df['image_path'].tolist()\n",
    "        text = clip.tokenize(text_prompt).to(self.device)\n",
    "        text_features = self.model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features.detach().cpu().numpy()\n",
    "        reference_embedding = text_features\n",
    "            \n",
    "        similarity_score = np.dot(reference_embedding, embeddings_np.T)/(np.linalg.norm(reference_embedding)*np.linalg.norm( embeddings_np, axis=1))\n",
    "        score_df = pd.DataFrame(data=list(similarity_score.T), columns=['similarity_score'])\n",
    "        score_df['image_path'] = image_paths \n",
    "        self.sorted_scores = score_df.sort_values(by='similarity_score', ascending=False)\n",
    "        image_name = text_prompt.replace(\" \",\"_\")\n",
    "        save_loc_sorted_scores = Path(self.sorted_scores_save_loc) / f\"{image_name}.csv\"\n",
    "        self.sorted_scores.to_csv(save_loc_sorted_scores, index=False)\n",
    "        return self.sorted_scores\n",
    "        \n",
    "        \n",
    "    def plot_image_grid(self, image_paths_df, tail=0  ,nrows=5, ncols=2):\n",
    "        \n",
    "        # Assume image_paths is your list of image file paths\n",
    "        if tail > 0:\n",
    "            image_paths_df = image_paths_df.tail(tail)\n",
    "        image_paths = image_paths_df['image_path'].tolist()\n",
    "        n_rows = min(nrows, int(image_paths_df.shape[0] // 2 ))\n",
    "        fig, axes = plt.subplots(n_rows, ncols, figsize=(10, nrows*5))\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            if i < len(image_paths)-1:\n",
    "                image_path = image_paths[i]\n",
    "                title = image_path.split(\"/\")[-1]\n",
    "                img = Image.open(image_path)\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(f'{title}  indx: {i}', fontsize=6)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def extract_embeddings(self): \n",
    "        self.prepare_images_path_df()\n",
    "        self.prepare_dataloader()\n",
    "        self.overwrite = True\n",
    "        self.build_embeddings()\n",
    "        self.save_embeddings()\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetname ='mannequin_in_dust_v0'\n",
    "aletheia_ds = Dataset.retrieve(name=dsetname)\n",
    "aletheia_df = aletheia_ds.to_dataframe()\n",
    "dataset_save_dir = os.environ['DATASET_PATH'] + \"/\" + dsetname\n",
    "if not os.path.exists(dataset_save_dir):\n",
    "    os.makedirs(name=dataset_save_dir)\n",
    "    aletheia_ds.download(dataset_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3154\n",
      "2023-07-08T01:43:09.443000\n",
      "17428\n",
      "2023-08-22T06:09:38.025000\n",
      "7375\n",
      "2023-08-22T06:09:38.025000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dsetname ='mannequin_in_dust_v0_dev'\n",
    "aletheia_ds = Dataset.retrieve(name=dsetname)\n",
    "aletheia_df = aletheia_ds.to_dataframe()Vjjj\n",
    "print(len(aletheia_df))\n",
    "print(max(aletheia_df['collected_on']))\n",
    "dsetname ='mannequin_in_dust_v0'\n",
    "aletheia_ds = Dataset.retrieve(name=dsetname)\n",
    "aletheia_df = aletheia_ds.to_dataframe()\n",
    "print(len(aletheia_df))\n",
    "print(max(aletheia_df['collected_on']))\n",
    "# dsetname ='mannequin_in_dust_night_dawn_10pos'\n",
    "# aletheia_ds = Dataset.retrieve(name=dsetname)\n",
    "# aletheia_df = aletheia_ds.to_dataframe()\n",
    "# print(len(aletheia_df))\n",
    "# print(max(aletheia_df['collected_on']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(save_dir, df):\n",
    "    save_dir = Path(save_dir)\n",
    "    dirs = [save_dir / d for d in df.id]#[df['label_human']].id]\n",
    "    image_paths = []\n",
    "    for i, dir in dirs:\n",
    "        if i % 10 == 0:\n",
    "            print(i / len(dir))\n",
    "        for fname in os.listdir(dir):\n",
    "            if 'debayeredrgb' in fname:\n",
    "                image_paths.append(str(dir / p))\n",
    "    return image_paths\n",
    "save_file = Path(dataset_save_dir) / \"image_ids.npy\"\n",
    "image_paths = np.load(save_file).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = ImageSimilarity(images_full_path=image_paths, data_base_path=dataset_save_dir, dataset_name=dsetname, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17428"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aletheia_df['collected_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/272.296875 [00:00<?, ?it/s]/tmp/ipykernel_72566/3190044770.py:12: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img_pil = self.transform(Image.fromarray(imageio.imread(img_path)))\n",
      "  3%|â–Ž         | 9/272.296875 [00:31<15:11,  3.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sim\u001b[39m.\u001b[39;49mextract_embeddings()\n\u001b[1;32m      2\u001b[0m embeddings_np, paths_df \u001b[39m=\u001b[39m sim\u001b[39m.\u001b[39mload_embeddings()\n",
      "Cell \u001b[0;32mIn[6], line 163\u001b[0m, in \u001b[0;36mImageSimilarity.extract_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_dataloader()\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverwrite \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_embeddings()\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_embeddings()\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 65\u001b[0m, in \u001b[0;36mImageSimilarity.build_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     inference_loader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_dataloader()\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(inference_loader), total\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal): \n\u001b[1;32m     66\u001b[0m         batch, paths \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m], batch[\u001b[39m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mDatasetpreparer.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     11\u001b[0m     img_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mloc[idx, \u001b[39m'\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m     img_pil \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mfromarray(imageio\u001b[39m.\u001b[39;49mimread(img_path)))\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Download and open the image        \u001b[39;00m\n\u001b[1;32m     14\u001b[0m     img_pil \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(img_pil)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/imageio/__init__.py:97\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"imread(uri, format=None, **kwargs)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[39mReads an image from the specified file. Returns a numpy array, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m    to see what arguments are available for a particular format.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     90\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStarting with ImageIO v3 the behavior of this function will switch to that of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m iio.v3.imread. To keep the current behavior (and make this warning disappear)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     95\u001b[0m )\n\u001b[0;32m---> 97\u001b[0m \u001b[39mreturn\u001b[39;00m imread_v2(uri, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/imageio/v2.py:360\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39m\u001b[39mri\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mimopen_args) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m--> 360\u001b[0m     result \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39;49mread(index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    362\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/imageio/plugins/pillow.py:224\u001b[0m, in \u001b[0;36mPillowPlugin.read\u001b[0;34m(self, index, mode, rotate, apply_gamma, writeable_output, pilmode, exifrotate, as_gray)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, \u001b[39mint\u001b[39m):\n\u001b[1;32m    222\u001b[0m     \u001b[39m# will raise IO error if index >= number of frames in image\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image\u001b[39m.\u001b[39mseek(index)\n\u001b[0;32m--> 224\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_transforms(\n\u001b[1;32m    225\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_image, mode, rotate, apply_gamma, writeable_output\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter(\n\u001b[1;32m    229\u001b[0m         mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    230\u001b[0m         rotate\u001b[39m=\u001b[39mrotate,\n\u001b[1;32m    231\u001b[0m         apply_gamma\u001b[39m=\u001b[39mapply_gamma,\n\u001b[1;32m    232\u001b[0m         writeable_output\u001b[39m=\u001b[39mwriteable_output,\n\u001b[1;32m    233\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/imageio/plugins/pillow.py:307\u001b[0m, in \u001b[0;36mPillowPlugin._apply_transforms\u001b[0;34m(self, image, mode, rotate, apply_gamma, writeable_output)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[39melse\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    304\u001b[0m         \u001b[39m# Let pillow know that it is okay to return 16-bit\u001b[39;00m\n\u001b[1;32m    305\u001b[0m         image\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m desired_mode\n\u001b[0;32m--> 307\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(image)\n\u001b[1;32m    309\u001b[0m meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata(index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image\u001b[39m.\u001b[39mtell(), exclude_applied\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    310\u001b[0m \u001b[39mif\u001b[39;00m rotate \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mOrientation\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m meta:\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/PIL/Image.py:696\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    694\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtobytes(\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    695\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtobytes()\n\u001b[1;32m    697\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e, (\u001b[39mMemoryError\u001b[39;00m, \u001b[39mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/PIL/Image.py:754\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mif\u001b[39;00m encoder_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m args \u001b[39m==\u001b[39m ():\n\u001b[1;32m    752\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[0;32m--> 754\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    756\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    757\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.8/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sim.extract_embeddings()\n",
    "embeddings_np, paths_df = sim.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_paths = [None for _ in range(n_images_final)]\n",
    "\n",
    "import random\n",
    "order = list(enumerate(kmeans.labels_))\n",
    "random.shuffle(order)\n",
    "\n",
    "for i, l in order:\n",
    "    if final_paths[l] == None:\n",
    "        final_paths[l] = paths_df.image_path.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imids = [p.split('_')[-1][:-4] for p in final_paths]\n",
    "fin_df = df[df.id.isin(imids)]\n",
    "print(len(fin_df))\n",
    "print(len(imids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aletheia_dataset_creator.dataset_tools.aletheia_dataset_helpers import imageids_to_dataset\n",
    "\n",
    "desc = f\"Mannequins standing in dust, diversified. ({len(imids)} images)\"\n",
    "imageids_to_dataset(image_ids=imids, dataset_name=\"mannequin_in_dust_v0_diverse\", dataset_description=desc, dataset_kind=Dataset.KIND_ANNOTATION, production_dataset=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
