{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexli/miniconda3/envs/cvml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "import datetime\n",
    "import io\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from brtdevkit.core.db.athena import AthenaClient\n",
    "from brtdevkit.data import Dataset\n",
    "from timezonefinder import TimezoneFinderL\n",
    "import pytz\n",
    "import cv2\n",
    "from brtdevkit.util.aws.s3 import S3\n",
    "client = S3()\n",
    "\n",
    "from aletheia_dataset_creator.dataset_tools.aletheia_dataset_helpers import imageids_to_dataset\n",
    "from aletheia_dataset_creator.config.dataset_config import LEFT_CAMERAS, ALL_CAMERA_PAIRS_LIST\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "athena = AthenaClient()\n",
    "s3 = boto3.resource('s3')\n",
    "tf = TimezoneFinderL()\n",
    "home = Path(os.path.expanduser('~'))\n",
    "data_path = Path(os.environ['OUTPUT_PATH']) / \"manny_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_drive_names = ['JUPD-061_2023-8-21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/alexli/results/manny_2/df_dusty_man.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m     query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mSELECT ij.id, hard_drive_name, robot_name, collected_on,\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m        bag_name, operating_field_name, operation_time, latitude, longitude, geohash, camera_location, \u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39m        bundle, group_id, s3_bucket, s3_key, special_notes\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m    WHERE \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhard_drive_name\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m IN (\u001b[39m\u001b[39m'\u001b[39m\u001b[39mJUPD-061_2023-8-21\u001b[39m\u001b[39m'\u001b[39m\u001b[39m) AND image_artifact_jupiter.kind = \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdebayeredrgb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m AND camera_location LIKE \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%le\u001b[39;00m\u001b[39mft\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     13\u001b[0m     df_dusty: pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m athena\u001b[39m.\u001b[39mget_df(query) \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     df_dusty\u001b[39m.\u001b[39;49mto_parquet(data_path \u001b[39m/\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdf_dusty_man.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m df_dusty[\u001b[39m'\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_dusty[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m df_dusty \u001b[39m=\u001b[39m df_dusty\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/pandas/util/_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/pandas/core/frame.py:2677\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2590\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2591\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2674\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2677\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2678\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2679\u001b[0m     path,\n\u001b[1;32m   2680\u001b[0m     engine,\n\u001b[1;32m   2681\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2682\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2683\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2684\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2685\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2686\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/pandas/io/parquet.py:416\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    414\u001b[0m path_or_buf: FilePathOrBuffer \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 416\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    417\u001b[0m     df,\n\u001b[1;32m    418\u001b[0m     path_or_buf,\n\u001b[1;32m    419\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    420\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    421\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    422\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    423\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    424\u001b[0m )\n\u001b[1;32m    426\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/pandas/io/parquet.py:175\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     from_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpreserve_index\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m index\n\u001b[1;32m    173\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_pandas(df, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfrom_pandas_kwargs)\n\u001b[0;32m--> 175\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    176\u001b[0m     path,\n\u001b[1;32m    177\u001b[0m     kwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mfilesystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    178\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    179\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    180\u001b[0m     is_dir\u001b[39m=\u001b[39;49mpartition_cols \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    181\u001b[0m )\n\u001b[1;32m    182\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[39mif\u001b[39;00m partition_cols \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         \u001b[39m# writes to multiple files under the given path\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/pandas/io/parquet.py:101\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     91\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     93\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[1;32m     94\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m    102\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/pandas/io/common.py:711\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    702\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    703\u001b[0m             handle,\n\u001b[1;32m    704\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    707\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    708\u001b[0m         )\n\u001b[1;32m    709\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 711\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    712\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    714\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/alexli/results/manny_2/df_dusty_man.parquet'"
     ]
    }
   ],
   "source": [
    "#https://bluerivertechnology.atlassian.net/wiki/spaces/JUPT/pages/3029106724/JQA-408+Post+Test+Report\n",
    "if os.path.exists(data_path / 'df_dusty_man.parquet'):\n",
    "    df_dusty = pd.read_parquet(data_path / 'df_dusty_man.parquet')\n",
    "else:\n",
    "    print(\"Cache miss\")\n",
    "    query = \"\"\"SELECT ij.id, hard_drive_name, robot_name, collected_on,\n",
    "        bag_name, operating_field_name, operation_time, latitude, longitude, geohash, camera_location, \n",
    "        bundle, group_id, s3_bucket, s3_key, special_notes\n",
    "    FROM image_jupiter AS ij\n",
    "    JOIN \"image_artifact_jupiter\" ON ij.\"id\" = \"image_artifact_jupiter\".\"image\"\n",
    "    WHERE \"hard_drive_name\" IN ('JUPD-061_2023-8-21') AND image_artifact_jupiter.kind = 'debayeredrgb' AND camera_location LIKE '%left'\n",
    "    \"\"\"\n",
    "    df_dusty: pd.DataFrame = athena.get_df(query) # type: ignore\n",
    "    df_dusty.to_parquet(data_path / 'df_dusty_man.parquet')\n",
    "df_dusty['image_id'] = df_dusty['id']\n",
    "df_dusty = df_dusty.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look through sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_id(df_row):\n",
    "    try:\n",
    "        return int(df_row['special_notes'].split(\" \")[-1])\n",
    "    except ValueError:\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(df_row, collected_on: str, folder_name: str):\n",
    "    if len(df_row) == 0:\n",
    "        whiteFrame = 255 * np.ones((604, 964, 3), np.uint8)\n",
    "        font = cv2.FONT_HERSHEY_PLAIN\n",
    "        whiteFrame = cv2.putText(whiteFrame, collected_on, (50, 400), font, 5, (0,0,0), 5)\n",
    "        return whiteFrame\n",
    "    elif isinstance(df_row, pd.DataFrame):\n",
    "        assert len(df_row) == 1\n",
    "        df_row = df_row.iloc[0]\n",
    "    file_name = Path(data_path) / folder_name / (str(df_row.image_id) + '.png')\n",
    "    if not os.path.exists(file_name):\n",
    "        client.download_file(df_row['s3_bucket'], df_row['s3_key'], file_name)\n",
    "    im = cv2.imread(str(file_name))\n",
    "    return im\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_frames(file_prefix: str, base_df: pd.DataFrame, folder_name: str):\n",
    "    \"\"\"\n",
    "    Given dictionary with image paths creates concatenated image and video and saves to output_dir.\n",
    "    \"\"\"\n",
    "    video_dir = Path(data_path) / 'videos' / str(file_prefix) \n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    video_name = video_dir / \"video.mp4\"\n",
    "    if os.path.exists(video_name):\n",
    "        return\n",
    "    writer = imageio.get_writer(video_name, fps=1)\n",
    "    k_df = base_df.sort_values('collected_on')\n",
    "    k_groups = base_df.groupby('group_id').groups\n",
    "    seen = set()\n",
    "    print(len(k_df))\n",
    "    for row in tqdm(k_df.iterrows()):\n",
    "        gid = row[1]['group_id']\n",
    "        if gid in seen:\n",
    "            continue\n",
    "        seen.add(gid)\n",
    "        values = k_groups[gid]\n",
    "        group = k_df.loc[values]\n",
    "        collected_on_str = str(group.iloc[0].collected_on)[11:19]\n",
    "        # try:\n",
    "        # concatenate image Horizontally\n",
    "        front_pod = np.concatenate(\n",
    "            (\n",
    "                get_image(group[group['camera_location'] == 'front-left-left'], collected_on_str, folder_name),\n",
    "                get_image(group[group['camera_location'] == 'front-center-left'], collected_on_str, folder_name),\n",
    "                get_image(group[group['camera_location'] == 'front-right-left'], collected_on_str, folder_name),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        rear_pod = np.concatenate(\n",
    "            (\n",
    "                get_image(group[group['camera_location'] == 'side-left-left'], collected_on_str, folder_name),\n",
    "                get_image(group[group['camera_location'] == 'rear-left'], collected_on_str, folder_name),\n",
    "                get_image(group[group['camera_location'] == 'side-right-left'], collected_on_str, folder_name),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        # concatenate image vertically\n",
    "        all_cameras = np.concatenate((front_pod, rear_pod), axis=0)[::4, ::4, ::-1]\n",
    "        # save concatenated image file\n",
    "        full_img_name = f\"{collected_on_str}.png\"\n",
    "        file_path = os.path.join(video_dir, full_img_name)\n",
    "        plt.imsave(file_path, all_cameras)\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        writer.append_data(imageio.imread(file_path))\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Skipping frame. Exception occurred: {e}\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look through dusty human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MF-blind-spot-b/w-front-rear--wheel-t', 'dust-Night-pos-6', 'Bundle 6777 object stop', 'test2-dust', 'MF-FC--FL/overlap-1', 'dust_pos7', 'dust-Night-pos-9', 'dust-dusk-pos1', 'MF---SL-rear/overlap-3', 'dust-dusk-pos3', 'CAT-sup-5', 'dust-Night-pos-10', 'dust_pos10-atmp-2', 'test-front-camera-2', 'MF-walk-across-camera-overlap-zone', 'aruco-3-dust-2', 'MF-walk-across-camera-overlap-zone-1', 'Test recording after network failure DELETE', 'dust-Night-pos-7', 'dust_pos8', 'MF-walk-across-camera-overlap-zone-2', 'dust_pos9', 'MF-walk-in-out-decision/over-lap-zone-1', 'MF-walk-across-camera-overlap-zone-4', 'dust_pos6', 'dust-dusk-pos5', 'MF-Run-into-blind-spot-1', 'dust-Night-pos-2', 'dust_pos6-atmp-2', 'apriltag-night-high-light', 'dust-Night-pos-1', 'dust-dusk-pos4', 'MF-FC--FL/overlap', 'dust-Night-pos-5', 'Human stop test 6738', 'MF-walk-in-out-decision/over-lap-zone', 'test-test-front-camera', 'MF-Run-into-blind-spot', 'dust_pos10', 'dust-Night-pos-8', 'dust-dusk-pos2', 'MF---SL-rear/overlap', 'MF---FL-SL/overlap', 'dust_pos9-atmp-2', 'dust-Night-pos-4', 'dust-Night-pos-3', 'MF-walk-across-camera-overlap-zone-3', 'MF---SL-rear/overlap-1', 'MF---SL-rear/overlap-2', 'test2-dust-2', <NA>}\n"
     ]
    }
   ],
   "source": [
    "print(set(df_dusty['special_notes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "from rich import pretty\n",
    "pretty.install()\n",
    "df_dusty = df_dusty[df_dusty['special_notes'].notna()]\n",
    "# https://bluerivertechnology.atlassian.net/wiki/spaces/JUPT/pages/3029106724/JQA-408+Post+Test+Report\n",
    "valid_notes = [\n",
    "    'dust-Night-pos-1',\n",
    "    'dust-Night-pos-2',\n",
    "    'dust-Night-pos-3',\n",
    "    'dust-Night-pos-4',\n",
    "    'dust-Night-pos-5',\n",
    "    'dust-Night-pos-6',\n",
    "    'dust-Night-pos-7',\n",
    "    'dust-Night-pos-8',\n",
    "    'dust-Night-pos-9',\n",
    "    'dust-Night-pos-10',\n",
    "    'dust-dusk-pos1',\n",
    "    'dust-dusk-pos2',\n",
    "    'dust-dusk-pos3',\n",
    "    'dust-dusk-pos4',\n",
    "    'dust-dusk-pos5',\n",
    "    'dust_pos6',\n",
    "    'dust_pos6-atmp-2',\n",
    "    'dust_pos7',\n",
    "    'dust_pos8',\n",
    "    'dust_pos9',\n",
    "    'dust_pos9-atmp-2',\n",
    "    'dust_pos10',\n",
    "    'dust_pos10-atmp-2'\n",
    "]\n",
    "print(len(valid_notes))\n",
    "df_dusty = df_dusty[df_dusty['special_notes'].isin(valid_notes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dusty = df_dusty.sort_values('collected_on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205509\n",
      "{'side-right-left', 'rear-left', 'front-right-left', 'front-left-left', 'front-center-left', 'side-left-left'}\n"
     ]
    }
   ],
   "source": [
    "print(len(df_dusty.sort_values('collected_on')))\n",
    "print(set(df_dusty['camera_location']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second(pddatetime):\n",
    "    return pddatetime.strftime('%Y/%m/%d %H:%M:%S')\n",
    "df_dusty['collected_second'] = df_dusty['collected_on'].apply(get_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_groups = df_dusty.groupby(['collected_second', 'camera_location']).first()['group_id']\n",
    "df_dusty_subset = df_dusty[df_dusty['group_id'].isin(ok_groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hard_drive_name</th>\n",
       "      <th>robot_name</th>\n",
       "      <th>collected_on</th>\n",
       "      <th>bag_name</th>\n",
       "      <th>operating_field_name</th>\n",
       "      <th>operation_time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>geohash</th>\n",
       "      <th>bundle</th>\n",
       "      <th>group_id</th>\n",
       "      <th>s3_bucket</th>\n",
       "      <th>s3_key</th>\n",
       "      <th>special_notes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collected_second</th>\n",
       "      <th>camera_location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2023/08/09 22:58:44</th>\n",
       "      <th>rear-left</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>side-left-left</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>side-right-left</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2023/08/09 22:58:45</th>\n",
       "      <th>rear-left</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>side-left-left</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2023/08/25 23:08:42</th>\n",
       "      <th>front-left-left</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>front-right-left</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rear-left</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>side-left-left</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>side-right-left</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51488 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  hard_drive_name  robot_name  \\\n",
       "collected_second    camera_location                                     \n",
       "2023/08/09 22:58:44 rear-left          1                1           1   \n",
       "                    side-left-left     1                1           1   \n",
       "                    side-right-left    1                1           1   \n",
       "2023/08/09 22:58:45 rear-left          1                1           1   \n",
       "                    side-left-left     1                1           1   \n",
       "...                                   ..              ...         ...   \n",
       "2023/08/25 23:08:42 front-left-left    1                1           1   \n",
       "                    front-right-left   1                1           1   \n",
       "                    rear-left          2                2           2   \n",
       "                    side-left-left     2                2           2   \n",
       "                    side-right-left    2                2           2   \n",
       "\n",
       "                                      collected_on  bag_name  \\\n",
       "collected_second    camera_location                            \n",
       "2023/08/09 22:58:44 rear-left                    1         1   \n",
       "                    side-left-left               1         1   \n",
       "                    side-right-left              1         1   \n",
       "2023/08/09 22:58:45 rear-left                    1         1   \n",
       "                    side-left-left               1         1   \n",
       "...                                            ...       ...   \n",
       "2023/08/25 23:08:42 front-left-left              1         1   \n",
       "                    front-right-left             1         1   \n",
       "                    rear-left                    2         2   \n",
       "                    side-left-left               2         2   \n",
       "                    side-right-left              2         2   \n",
       "\n",
       "                                      operating_field_name  operation_time  \\\n",
       "collected_second    camera_location                                          \n",
       "2023/08/09 22:58:44 rear-left                            1               1   \n",
       "                    side-left-left                       1               1   \n",
       "                    side-right-left                      1               1   \n",
       "2023/08/09 22:58:45 rear-left                            1               1   \n",
       "                    side-left-left                       1               1   \n",
       "...                                                    ...             ...   \n",
       "2023/08/25 23:08:42 front-left-left                      1               1   \n",
       "                    front-right-left                     1               1   \n",
       "                    rear-left                            2               2   \n",
       "                    side-left-left                       2               2   \n",
       "                    side-right-left                      2               2   \n",
       "\n",
       "                                      latitude  longitude  geohash  bundle  \\\n",
       "collected_second    camera_location                                          \n",
       "2023/08/09 22:58:44 rear-left                1          1        1       1   \n",
       "                    side-left-left           1          1        1       1   \n",
       "                    side-right-left          1          1        1       1   \n",
       "2023/08/09 22:58:45 rear-left                1          1        1       1   \n",
       "                    side-left-left           1          1        1       1   \n",
       "...                                        ...        ...      ...     ...   \n",
       "2023/08/25 23:08:42 front-left-left          1          1        1       1   \n",
       "                    front-right-left         1          1        1       1   \n",
       "                    rear-left                2          2        2       2   \n",
       "                    side-left-left           2          2        2       2   \n",
       "                    side-right-left          2          2        2       2   \n",
       "\n",
       "                                      group_id  s3_bucket  s3_key  \\\n",
       "collected_second    camera_location                                 \n",
       "2023/08/09 22:58:44 rear-left                1          1       1   \n",
       "                    side-left-left           1          1       1   \n",
       "                    side-right-left          1          1       1   \n",
       "2023/08/09 22:58:45 rear-left                1          1       1   \n",
       "                    side-left-left           1          1       1   \n",
       "...                                        ...        ...     ...   \n",
       "2023/08/25 23:08:42 front-left-left          1          1       1   \n",
       "                    front-right-left         1          1       1   \n",
       "                    rear-left                2          2       2   \n",
       "                    side-left-left           2          2       2   \n",
       "                    side-right-left          2          2       2   \n",
       "\n",
       "                                      special_notes  \n",
       "collected_second    camera_location                  \n",
       "2023/08/09 22:58:44 rear-left                     1  \n",
       "                    side-left-left                1  \n",
       "                    side-right-left               1  \n",
       "2023/08/09 22:58:45 rear-left                     1  \n",
       "                    side-left-left                1  \n",
       "...                                             ...  \n",
       "2023/08/25 23:08:42 front-left-left               1  \n",
       "                    front-right-left              1  \n",
       "                    rear-left                     2  \n",
       "                    side-left-left                2  \n",
       "                    side-right-left               2  \n",
       "\n",
       "[51488 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dusty_subset.groupby(['collected_second', 'camera_location']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84387\n",
      "84387\n"
     ]
    }
   ],
   "source": [
    "print(len(df_dusty))\n",
    "print(len(df_dusty_subset))\n",
    "df_dusty = df_dusty_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2480 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m base_df \u001b[39m=\u001b[39m df_dusty[df_dusty[\u001b[39m'\u001b[39m\u001b[39mspecial_notes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m key]\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m, df_row \u001b[39min\u001b[39;00m tqdm(base_df\u001b[39m.\u001b[39miterrows(), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(base_df)):\n\u001b[0;32m----> 8\u001b[0m     file_name \u001b[39m=\u001b[39m folder_name \u001b[39m/\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(file_name):\n\u001b[1;32m     10\u001b[0m         client\u001b[39m.\u001b[39mdownload_file(df_row[\u001b[39m'\u001b[39m\u001b[39ms3_bucket\u001b[39m\u001b[39m'\u001b[39m], df_row[\u001b[39m'\u001b[39m\u001b[39ms3_key\u001b[39m\u001b[39m'\u001b[39m], file_name)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "for key in valid_notes:\n",
    "    folder_name = Path(data_path) / 'humans_in_dust' / key\n",
    "    if os.path.exists(data_path / 'videos' / key / 'video.mp4'):\n",
    "        continue\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    base_df = df_dusty[df_dusty['special_notes'] == key]\n",
    "    for id, df_row in tqdm(base_df.iterrows(), total=len(base_df)):\n",
    "        file_name = folder_name / str(id + '.png')\n",
    "        if not os.path.exists(file_name):\n",
    "            client.download_file(df_row['s3_bucket'], df_row['s3_key'], file_name)\n",
    "    create_video_frames(key, base_df=base_df, folder_name=f'humans_in_dust/{key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_notes = [\n",
    "    'dust-Night-pos-1',\n",
    "    'dust-Night-pos-2',\n",
    "    'dust-Night-pos-3',\n",
    "    'dust-Night-pos-4',\n",
    "    'dust-Night-pos-5',\n",
    "    'dust-Night-pos-6',\n",
    "    'dust-Night-pos-7',\n",
    "    'dust-Night-pos-8',\n",
    "    'dust-Night-pos-9',\n",
    "    'dust-Night-pos-10',\n",
    "    'dust-dusk-pos1',\n",
    "    'dust-dusk-pos2',\n",
    "    'dust-dusk-pos3',\n",
    "    'dust-dusk-pos4',\n",
    "    'dust-dusk-pos5',\n",
    "    'dust_pos6',\n",
    "    'dust_pos6-atmp-2',\n",
    "    'dust_pos7',\n",
    "    'dust_pos8',\n",
    "    'dust_pos9',\n",
    "    'dust_pos9-atmp-2',\n",
    "    'dust_pos10',\n",
    "    'dust_pos10-atmp-2'\n",
    "]\n",
    "\n",
    "only_human_images = {\n",
    "    'dust-Night-pos-1': {\n",
    "        'front-left-left': [('05:02:00', '05:02:30'), ('05:06:00', '05:07:00'), ('05:07:20', '05:07:30')],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [('05:02:00', '05:02:30')],\n",
    "        'side-right-left': [],\n",
    "        'rear-left': []\n",
    "    },\n",
    "    'dust-Night-pos-2': {\n",
    "        'front-left-left': [('05:07:30', '05:08:30')],\n",
    "        'front-center-left': [('05:11:30', '05:11:40'),('05:12:40', '05:13:10') ],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-Night-pos-3': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('05:13:15', '05:13:40'), ('05:17:00', '05:18:40')],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-Night-pos-4': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('05:18:00', '05:19:05'), ('05:22:30', '05:22:50')],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-Night-pos-5': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('05:23:45', '05:27:00')],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [('05:29:10', '05:31:10')],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-Night-pos-6': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [('05:31:11', '05:32:30'), ('05:33:05', '05:34:50')],\n",
    "        'rear-left': [('05:33:05', '05:34:50')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-Night-pos-7': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('05:49:32', '05:51:30')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-Night-pos-8': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('05:51:30', '05:51:40'), ('05:53:10', '05:54:20'), ('05:59:00', '05:59:59')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-Night-pos-9': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('05:59:20', '06:03:00'), ('06:04:50', '06:06:35')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-Night-pos-10': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('06:07:50', '06:09:40')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-dusk-pos1': {\n",
    "        'front-left-left': [('02:09:00','02:17:30')],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [('02:09:00','02:12:30')],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-dusk-pos2': {\n",
    "        'front-left-left': [('02:17:28','02:20:30')],\n",
    "        'front-center-left': [('02:20:21', '02:23:45')],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-dusk-pos3': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('02:23:40', '02:26:00'), ('02:26:00', '02:35:00')],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-dusk-pos4': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('02:34:00', '02:40:45')],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust-dusk-pos5': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('02:40:45', '02:42:25')],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust_pos6': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [('22:57:03', '22:58:59')],\n",
    "        'rear-left': [],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust_pos6-atmp-2': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('23:04:00', '23:06:00')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust_pos7': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [('23:18:02', '23:20:00')],\n",
    "        'rear-left': [('23:18:02','23:27:00')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust_pos8': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('23:27:00', '23:31:00')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust_pos9': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('23:31:00', '23:34:43')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust_pos9-atmp-2': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('23:34:43', '23:44:00')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust_pos10': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('23:44:00', '23:59:59')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "    'dust_pos10-atmp-2': {\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': [],\n",
    "        'side-left-left': [],\n",
    "        'rear-left': [('23:58:00', '23:59:59')],\n",
    "        'side-right-left': [],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def filter_movie(movie, start_t, end_t):\n",
    "    timestamp = movie.iloc[0]['collected_on']\n",
    "    s_hour, s_minute, s_second = map(int, start_t.split(':'))\n",
    "    e_hour, e_minute, e_second = map(int, end_t.split(':'))\n",
    "    y, m, d = timestamp.year, timestamp.month, timestamp.day\n",
    "    tzinfo = movie['collected_on'].iloc[0].tzinfo\n",
    "    start_dt = datetime(y, m, d, s_hour, s_minute, s_second, tzinfo=tzinfo)\n",
    "    end_dt = datetime(y, m, d, e_hour, e_minute, e_second, tzinfo=tzinfo)\n",
    "    return movie[(start_dt < movie['collected_on']) & (movie['collected_on'] < end_dt)]\n",
    "\n",
    "cleaned_human_runs = []\n",
    "for notes in valid_notes:\n",
    "    for camera_location in ['front-left-left', 'front-center-left', 'front-right-left', 'side-left-left', 'rear-left', 'side-right-left']:\n",
    "        times = only_human_images[notes][camera_location]\n",
    "        for start_t, end_t in times:\n",
    "            sequence = df_dusty.loc[(df_dusty['special_notes'] == notes) & (df_dusty['camera_location'] == camera_location)]\n",
    "            filtered = filter_movie(sequence, start_t, end_t)\n",
    "            # print(len(filtered))\n",
    "            if not len(filtered):\n",
    "                print(notes, start_t, end_t) # oops\n",
    "            cleaned_human_runs.append(filtered)\n",
    "human_dusty_df = pd.concat(cleaned_human_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.create(\n",
    "    name='mannequin_in_dust_night_dawn_10pos',\n",
    "    description=\"sequences of a mannequin with dust blowing around it from 10 positions at night and dusk. All images contain a mannequin. Collected Aug 22 2023. Left cameras only (7375 images)\",\n",
    "    kind=Dataset.KIND_IMAGE,\n",
    "    image_ids=list(human_dusty_df['image_id']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create annotated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1650, 108)\n",
      "(7375, 107)\n",
      "(9025, 109)\n"
     ]
    }
   ],
   "source": [
    "aletheia_ds = Dataset.retrieve(name='mannequin_in_dust')\n",
    "aletheia_df_1 = aletheia_ds.to_dataframe()\n",
    "print(aletheia_df_1.shape)\n",
    "# retrieve dataset from aletheia\n",
    "aletheia_ds = Dataset.retrieve(name='mannequin_in_dust_night_dawn_10pos')\n",
    "aletheia_df_2 = aletheia_ds.to_dataframe()\n",
    "print(aletheia_df_2.shape)\n",
    "aletheia_df = pd.concat([aletheia_df_1, aletheia_df_2])\n",
    "print(aletheia_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aletheia_df['image_id'] = aletheia_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_slow(from_df, name, description, kind='image') -> None:\n",
    "    imids = list(from_df['image_id'])\n",
    "    desc = f\"{description} ({len(from_df['image_id'])} images)\"\n",
    "    print(len(imids))\n",
    "    from_df.to_parquet(data_path / '{name}.parquet', index=False)\n",
    "    imageids_to_dataset(imids, name, dataset_description=desc, dataset_kind=kind, production_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9025\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "Warning 311 images do not have a corresponding annotation.\n",
      "Preparing stereo dataframe...\n",
      "Size of left dataframe: 8714\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "Size of stereo dataframe: 8714\n",
      "Sending 8714 annotated_ids for creating dataset\n",
      "Time taken to prepare data for dataset creation job: 1.80 mins\n"
     ]
    }
   ],
   "source": [
    "make_dataset_slow(from_df=aletheia_df, \n",
    "    name='mannequin_in_dust_v0',\n",
    "    description=\"A mannequin with dust billowing around. All images contain a mannequin.\",\n",
    "    kind=Dataset.KIND_ANNOTATION,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
