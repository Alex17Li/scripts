{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596384ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import cv2\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import pickle\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from datetime import datetime, timedelta, date\n",
    "# from geopy.geocoders import Nominatim\n",
    "# from geopy.extra.rate_limiter import RateLimiter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speed(df_row):\n",
    "    gps_ast = ast.literal_eval(df_row['gps_can_data'])\n",
    "    if 'speed' in gps_ast:\n",
    "        return gps_ast['speed']\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_meta_with_pred(master_df, label_df, unlabeled_pred_df, labeled_pred_df):\n",
    "    print('master_df', master_df.shape)\n",
    "    df = master_df[['id', 'collected_on', 'camera_location', 'operation_time', 'geohash']]\n",
    "    \n",
    "    # merge with label_df\n",
    "    df = df.merge(label_df, on='id', how='left')\n",
    "    df = df.fillna(0)\n",
    "    print('merge label counts', df.shape)\n",
    "\n",
    "    # load prediction on unlabeled data to get dust ratios\n",
    "    print('unlabeled_pred_df', unlabeled_pred_df.shape)\n",
    "    if not 'total_averaged_dust_ratio' in unlabeled_pred_df and 'total_averaged_dust_conf' in unlabeled_pred_df:\n",
    "        unlabeled_pred_df['total_averaged_dust_ratio'] = unlabeled_pred_df['total_averaged_dust_conf']\n",
    "        unlabeled_pred_df['triangle_averaged_dust_ratio'] = unlabeled_pred_df['masked_avg_dust_conf']\n",
    "        df = df.merge(unlabeled_pred_df[['id', 'total_averaged_dust_ratio', 'triangle_averaged_dust_ratio']], on='id')\n",
    "    if 'total_averaged_dust_ratio' in unlabeled_pred_df:\n",
    "        df = df.merge(unlabeled_pred_df[['id', 'total_averaged_dust_ratio', 'triangle_averaged_dust_ratio']], on='id')\n",
    "    if 'pred_dust_ratio' in unlabeled_pred_df:  # GRETZKY_1385_dust_head branch\n",
    "        unlabeled_pred_df['pred_dust_ratio'] /= 100\n",
    "        df = df.merge(unlabeled_pred_df[['id', 'pred_dust_ratio']], on='id')\n",
    "    print('merge unlabeled dust ratios', df.shape)\n",
    "\n",
    "    # load prediction on labeled data to get the prediction \"state\"\n",
    "    print('labeled_pred_df', labeled_pred_df.shape)\n",
    "    # convert LO states to regular states and fill empty states with TNs\n",
    "    df = df.merge(labeled_pred_df[['id', 'state']], on='id', how='left').drop_duplicates(subset=['id'])\n",
    "    df = df.fillna('true_negative')\n",
    "    df = df.replace('large_object_true_positive', 'true_positive')\n",
    "    df = df.replace('large_object_false_negative', 'false_negative')\n",
    "    print('merge labeled states', df.shape)\n",
    "\n",
    "    # sort by time and add datetime column\n",
    "    df = df.sort_values('collected_on')\n",
    "    df['datetime'] = df.collected_on.apply(datetime.fromisoformat)\n",
    "    df['datehm'] = df.collected_on.apply(lambda x:str(x)[:16])\n",
    "    print('final_df', df.shape)\n",
    "    print('# TPs', len(df[df.state == 'true_positive']), '# Positives', len(df[(df.state == 'true_positive') | (df.state == 'false_negative')]))\n",
    "    df['speed'] = master_df.apply(get_speed, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259fc09",
   "metadata": {},
   "source": [
    "## Load example df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = '/data/jupiter/li.yu/data'\n",
    "unlabeled_datasets = [\"Jupiter_2023_03_29_10pm_30_3pm_Loamy_812_stops_stereo_2\", \n",
    "                      \"Jupiter_2023_04_05_loamy869_dust_collection_stereo\", \n",
    "                      \"Jupiter_2023_may_loamy731_vehicle_dust_human_stereo\"]\n",
    "labeled_datasets = [[\"Jupiter_2023_03_02_and_2930_human_vehicle_in_dust_labeled\", \n",
    "                    \"Jupiter_2023_March_29th30th_human_vehicle_in_dust_front_pod_labeled\"], \n",
    "                    [\"Jupiter_2023_04_05_loamy869_dust_collection_stereo_labeled\"], \n",
    "                    [\"Jupiter_2023_may_loamy731_vehicle_dust_human_stereo_labeled\"]]\n",
    "pred_root = '/data/jupiter/li.yu/exps/driveable_terrain_model/'\n",
    "train_id = 'dust_51_v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30'\n",
    "# train_id = 'v57rd_4cls_tiny0occluded5reverse5triangle5_msml_0305'\n",
    "rear_pod_model = False   # True for 4-class, False for 7 or 8 class models\n",
    "\n",
    "# set 1\n",
    "dfs = []\n",
    "for i in range(len(labeled_datasets)):\n",
    "    master_df = pd.read_csv(os.path.join(data_root_dir, unlabeled_datasets[i], 'master_annotations.csv'), low_memory=False)\n",
    "    label_df_unmerged = [\n",
    "        pd.read_csv(os.path.join(data_root_dir, labeled_datasets[i][j], 'label_count.csv'))\n",
    "        for j in range(len(labeled_datasets[i]))\n",
    "    ]\n",
    "    label_df = pd.concat(label_df_unmerged, ignore_index=True)\n",
    "    if rear_pod_model:\n",
    "        unlabeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, unlabeled_datasets[i], 'dust_ratio.csv'))\n",
    "    else:\n",
    "        unlabeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, unlabeled_datasets[i], 'output.csv'))\n",
    "    labeled_pred_df_unmerged = [\n",
    "        pd.read_csv(os.path.join(pred_root, train_id, labeled_datasets[i][j], 'output.csv'))\n",
    "        for j in range(len(labeled_datasets[i]))\n",
    "    ]\n",
    "    labeled_pred_df = pd.concat(labeled_pred_df_unmerged, ignore_index=True)\n",
    "    dfs.append(merge_meta_with_pred(master_df, label_df, unlabeled_pred_df, labeled_pred_df))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print('merge all three sets (double df3 as it is halved) to get df', df.shape)\n",
    "pdf = df[(df.state == 'true_positive') | (df.state == 'false_negative')]\n",
    "print('positive images', pdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 class models\n",
    "train_id = 'v57rd_4cls_tiny0occluded5reverse5triangle5_msml_0305'\n",
    "unlabeled_datasets = [\"Jupiter_2023_03_29_10pm_30_3pm_Loamy_812_stops_stereo_2\", \n",
    "                      \"Jupiter_2023_04_05_loamy869_dust_collection_stereo\", \n",
    "                      \"Jupiter_2023_may_loamy731_vehicle_dust_human_stereo\"]\n",
    "labeled_datasets = [\"Jupiter_2023_03_02_and_2930_human_vehicle_in_dust_labeled\", \n",
    "                    \"Jupiter_2023_March_29th30th_human_vehicle_in_dust_front_pod_labeled\", \n",
    "                    \"Jupiter_2023_04_05_loamy869_dust_collection_stereo_labeled\", \n",
    "                    \"Jupiter_2023_may_loamy731_vehicle_dust_human_stereo_labeled\"]\n",
    "\n",
    "# set 1\n",
    "i = 0\n",
    "master_df = pd.read_csv(os.path.join(data_root_dir, unlabeled_datasets[i], 'master_annotations.csv'), low_memory=False)\n",
    "label_df1 = pd.read_csv(os.path.join(data_root_dir, labeled_datasets[i], 'label_count.csv'))\n",
    "label_df2 = pd.read_csv(os.path.join(data_root_dir, labeled_datasets[i+1], 'label_count.csv'))\n",
    "label_df = pd.concat([label_df1, label_df2], ignore_index=True)\n",
    "unlabeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, unlabeled_datasets[i]+'_epoch43_newmask', 'dust_ratio.csv'))\n",
    "labeled_pred_df1 = pd.read_csv(os.path.join(pred_root, train_id, labeled_datasets[i]+'_epoch43', 'output.csv'))\n",
    "labeled_pred_df2 = pd.read_csv(os.path.join(pred_root, train_id, labeled_datasets[i+1]+'_epoch43', 'output.csv'))\n",
    "labeled_pred_df = pd.concat([labeled_pred_df1, labeled_pred_df2], ignore_index=True)\n",
    "df1 = merge_meta_with_pred(master_df, label_df, unlabeled_pred_df, labeled_pred_df)\n",
    "\n",
    "# set 2\n",
    "i = 1\n",
    "master_df = pd.read_csv(os.path.join(data_root_dir, unlabeled_datasets[i], 'master_annotations.csv'), low_memory=False)\n",
    "label_df = pd.read_csv(os.path.join(data_root_dir, labeled_datasets[i+1], 'label_count.csv'))\n",
    "unlabeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, unlabeled_datasets[i]+'_epoch43', 'dust_ratio.csv'))\n",
    "labeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, labeled_datasets[i+1]+'_epoch43', 'output.csv'))\n",
    "df2 = merge_meta_with_pred(master_df, label_df, unlabeled_pred_df, labeled_pred_df)\n",
    "\n",
    "# set 3\n",
    "i = 2\n",
    "master_df = pd.read_csv(os.path.join(data_root_dir, unlabeled_datasets[i], 'master_annotations.csv'), low_memory=False)\n",
    "label_df = pd.read_csv(os.path.join(data_root_dir, labeled_datasets[i+1], 'label_count.csv'))\n",
    "unlabeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, unlabeled_datasets[i]+'_epoch43', 'dust_ratio.csv'))\n",
    "labeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, labeled_datasets[i+1]+'_epoch43', 'output.csv'))\n",
    "df3 = merge_meta_with_pred(master_df, label_df, unlabeled_pred_df, labeled_pred_df)\n",
    "\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "print('merge all three sets (double df3 as it is halved) to get df', df.shape)\n",
    "pdf = df[(df.state == 'true_positive') | (df.state == 'false_negative')]\n",
    "print('positive images', pdf.shape)\n",
    "\n",
    "# df = df2\n",
    "dfs = [df1, df2, df3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6624877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/alexli/logs/summary_7204_vehicles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db996155",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = ['rear', 'side']\n",
    "cameras = ['rear_side']\n",
    "windows = ['_5s', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7a44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)\n",
    "df['total_averaged_dust_ratio'] = df['dust_percent']\n",
    "df['triangle_averaged_dust_ratio'] = df['dust_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(row):\n",
    "    pred = row['predicted_stop_class']\n",
    "    label = row['label_stop_class']\n",
    "    if pred and label:\n",
    "        if label:\n",
    "            return 'true_positive'\n",
    "        else:\n",
    "            return 'false_positive'\n",
    "    else:\n",
    "        if label:\n",
    "            return 'false_negative'\n",
    "        else:\n",
    "            return 'false_positive'\n",
    "\n",
    "df['state'] = df.apply(get_state,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ad87b",
   "metadata": {},
   "source": [
    "## Extract average/maximum delta_t between two successive TPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03027898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(df, interval=5*60, per_camera=False):\n",
    "    df = df.sort_values('collected_on')\n",
    "    df['datetime'] = df.collected_on.apply(datetime.fromisoformat)\n",
    "    sequence_dfs = []\n",
    "    delta = timedelta(seconds=interval)\n",
    "    start = True\n",
    "    i0, i = 0, 0\n",
    "    while i < len(df):\n",
    "        if start:\n",
    "            t0 = df.iloc[i].datetime\n",
    "            start = False\n",
    "        else:\n",
    "            t1 = df.iloc[i].datetime\n",
    "            if t1 - t0 > delta or i == len(df) - 1:\n",
    "                chunk_df = df.iloc[i0 : i if i < len(df) - 1 else len(df)]\n",
    "                if per_camera:\n",
    "                    camera_locations = chunk_df.camera_location.unique()\n",
    "                    camera_locations.sort()\n",
    "                    for camera_location in camera_locations:\n",
    "                        sequence_df = chunk_df[chunk_df.camera_location == camera_location]\n",
    "                        sequence_df = sequence_df.sort_values('collected_on')\n",
    "                        sequence_dfs.append(sequence_df)\n",
    "                else:\n",
    "                    sequence_dfs.append(chunk_df)\n",
    "                start = True\n",
    "                i0 = i\n",
    "            else:\n",
    "                t0 = t1\n",
    "        i += 1\n",
    "    return sequence_dfs\n",
    "\n",
    "def get_avg_dust_ratio_and_delta_t_helper(df, states, cameras, window_sizes, step_sizes):\n",
    "    def get_results_for_a_df(df, camera, state_col_idx=13, t=None, window=None):\n",
    "        avg_dust_ratio, avg_delta_t, max_delta_t = None, None, None\n",
    "        dust_key = 'triangle_averaged_dust_ratio' if camera == 'rear' else 'total_averaged_dust_ratio'\n",
    "        tp_df = df[df.state == 'true_positive']\n",
    "        if len(tp_df) == 0:\n",
    "            avg_dust_ratio = df[dust_key].mean()  # avg dust ratio of TNs and TPs\n",
    "        else:\n",
    "            avg_dust_ratio = tp_df[dust_key].mean()  # avg dust ratio of TPs\n",
    "        # set begin and end states of df to TP\n",
    "        if t is not None and len(tp_df) == 0 and \\\n",
    "            (df.iloc[0].datetime - t).total_seconds() < 0.5 and \\\n",
    "            (t + window - df.iloc[-1].datetime).total_seconds() < 0.5:\n",
    "            avg_delta_t = max_delta_t = window.total_seconds() + 2\n",
    "#             print(t, (df.iloc[0].datetime - t).total_seconds(), (t + window - df.iloc[-1].datetime).total_seconds())\n",
    "        else:\n",
    "            df_copy = df.copy()\n",
    "            df_copy.iat[0, state_col_idx] = df_copy.iat[len(df_copy)-1, state_col_idx] = 'true_positive'\n",
    "            df_copy = df_copy[df_copy.state == 'true_positive']\n",
    "            if len(df_copy) > 1:\n",
    "                delta_ts = [(df_copy.iloc[i].datetime - df_copy.iloc[i-1].datetime).total_seconds() for i in range(1, len(df_copy))]\n",
    "                avg_delta_t = sum(delta_ts) / len(delta_ts)\n",
    "                max_delta_t = max(delta_ts)\n",
    "        return avg_dust_ratio, avg_delta_t, max_delta_t\n",
    "    \n",
    "    def get_avg_max_dust_ratio(df, interval):\n",
    "        max_dust_ratios = []\n",
    "        seq_dfs = get_sequences(df, interval, per_camera=False)\n",
    "        for seq_df in seq_dfs:\n",
    "            cameras = seq_df.camera_location.to_list()\n",
    "            take_every = False\n",
    "            if len(set(cameras)) < len(cameras):\n",
    "                take_every = True\n",
    "            dust_ratios = []\n",
    "            for i,row in seq_df.iterrows():\n",
    "#                 if row.state == 'true_positive':\n",
    "                if row.camera_location.startswith('rear'):\n",
    "                    dust_ratio = row.triangle_averaged_dust_ratio\n",
    "                else:\n",
    "                    dust_ratio = row.total_averaged_dust_ratio\n",
    "                dust_ratios.append(dust_ratio)\n",
    "            if dust_ratios:\n",
    "                if take_every:\n",
    "                    max_dust_ratios += dust_ratios\n",
    "                else:\n",
    "                    max_dust_ratios.append(max(dust_ratios))\n",
    "        if max_dust_ratios:\n",
    "            return sum(max_dust_ratios) / len(max_dust_ratios)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def process_by_sliding_window(df, states, camera, window_size, step_size):\n",
    "        window = timedelta(seconds=window_size)\n",
    "        step = timedelta(seconds=step_size)\n",
    "        ts = df.iloc[0].datetime\n",
    "        te = df.iloc[-1].datetime\n",
    "        t = ts\n",
    "        window_states = defaultdict(list)\n",
    "        while t + window <= te:\n",
    "            sub_df = df[(df.datetime >= t) & (df.datetime < t+window)]\n",
    "            if len(sub_df) <= 2:\n",
    "                t += step\n",
    "                continue\n",
    "            avg_dust_ratio, avg_delta_t, max_delta_t = get_results_for_a_df(sub_df, camera, state_col_idx, t, window)\n",
    "            if camera == 'rear_side':\n",
    "                avg_dust_ratio = get_avg_max_dust_ratio(sub_df, 0.1)\n",
    "            if avg_dust_ratio is not None:\n",
    "                window_states[f'avg_dust_ratio_{camera}_{window_size}s'].append(avg_dust_ratio)\n",
    "                window_states[f'avg_delta_t_{camera}_{window_size}s'].append(avg_delta_t if avg_delta_t is not None else window_size)\n",
    "                window_states[f'max_delta_t_{camera}_{window_size}s'].append(max_delta_t if max_delta_t is not None else window_size)\n",
    "            t += step\n",
    "        states[f'avg_dust_ratio_{camera}_{window_size}s'].append(window_states[f'avg_dust_ratio_{camera}_{window_size}s'])\n",
    "        states[f'avg_delta_t_{camera}_{window_size}s'].append(window_states[f'avg_delta_t_{camera}_{window_size}s'])\n",
    "        states[f'max_delta_t_{camera}_{window_size}s'].append(window_states[f'max_delta_t_{camera}_{window_size}s'])\n",
    "    \n",
    "    state_col_idx = df.columns.values.tolist().index('state')\n",
    "    for camera in cameras:\n",
    "        # overall metrics\n",
    "        avg_dust_ratio = None\n",
    "        if camera == 'rear_side':\n",
    "            camera_df = df\n",
    "        else:\n",
    "            camera_df = df[df.camera_location.str.startswith(camera)]\n",
    "        if len(camera_df) > 2:\n",
    "            avg_dust_ratio, avg_delta_t, max_delta_t = get_results_for_a_df(camera_df, camera, state_col_idx)\n",
    "            if camera == 'rear_side':\n",
    "                avg_dust_ratio = get_avg_max_dust_ratio(camera_df, 0.1)\n",
    "\n",
    "            assert avg_dust_ratio is not None \n",
    "            if avg_dust_ratio is not None:\n",
    "                states[f'avg_dust_ratio_{camera}'].append(avg_dust_ratio)\n",
    "                states[f'avg_delta_t_{camera}'].append(avg_delta_t)\n",
    "                states[f'max_delta_t_{camera}'].append(max_delta_t)\n",
    "            # process in sliding windows\n",
    "            for window_size, step_size in zip(window_sizes, step_sizes):\n",
    "                process_by_sliding_window(camera_df, states, camera, window_size, step_size)\n",
    "    return states\n",
    "\n",
    "def get_avg_dust_ratio_and_delta_t(df, window_size=5, step_size=1):\n",
    "    tp_states = {}\n",
    "    window_sizes = [0, window_size]\n",
    "    step_sizes = [0, step_size]\n",
    "    for camera in cameras:\n",
    "        for window_size in window_sizes:\n",
    "            window_key = f'_{window_size}s' if window_size > 0 else ''\n",
    "            tp_states[f'avg_dust_ratio_{camera}{window_key}'] = []\n",
    "            tp_states[f'avg_delta_t_{camera}{window_key}'] = []\n",
    "            tp_states[f'max_delta_t_{camera}{window_key}'] = []\n",
    "    tpfn_df = df[(df.state == 'true_positive') | (df.state == 'false_negative')]\n",
    "    print('  tpfn_df', tpfn_df.shape)\n",
    "    if len(tpfn_df) > 2:\n",
    "        get_avg_dust_ratio_and_delta_t_helper(tpfn_df, tp_states, cameras, window_sizes[1:], step_sizes[1:])\n",
    "    return tp_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = {\n",
    "    '2023-03-29T22:23:46.331000': [0,-1], '2023-03-29T22:25:47.441000': [-2,-1], '2023-03-29T22:27:43.387000': [], '2023-03-29T22:29:30.005000': [-1], \n",
    "    '2023-03-29T22:32:39.504000': [], '2023-03-29T22:34:23.394000': [], '2023-03-29T22:36:12.145000': [-1], '2023-03-29T22:38:10.259000': [], \n",
    "    '2023-03-29T22:45:01.713000': [-10000], '2023-03-29T23:45:30.666000': [0,-1], '2023-03-29T23:47:11.274000': [-1], '2023-03-29T23:49:12.248000': [], \n",
    "    '2023-03-29T23:51:06.362000': [], '2023-03-29T23:55:11.838000': [0], '2023-03-29T23:57:10.445000': [], '2023-03-29T23:58:54.361000': [], \n",
    "    '2023-03-30T00:00:58.719000': [], '2023-03-30T00:05:36.223000': [], '2023-03-30T00:07:33.158000': [], '2023-03-30T00:09:19.825000': [], \n",
    "    '2023-03-30T00:11:10.935000': [], '2023-03-30T00:11:26.189000': [], '2023-03-30T00:11:43.140000': [], '2023-03-30T00:12:00.133000': [], \n",
    "    '2023-03-30T00:21:15.258000': [-10000], '2023-03-30T00:28:39.895000': [-10000], '2023-03-30T00:32:33.586000': [-10000], '2023-03-30T01:30:55.446000': [], \n",
    "    '2023-03-30T01:31:09.871000': [], '2023-03-30T01:32:35.611000': [], '2023-03-30T01:32:52.360000': [], '2023-03-30T01:34:33.331000': [], \n",
    "    '2023-03-30T01:36:20.737000': [], '2023-03-30T01:36:39.402000': [], '2023-03-30T01:37:16.698000': [], '2023-03-30T01:37:32.989000': [], \n",
    "    '2023-03-30T01:37:48.514000': [], '2023-03-30T01:42:00.564000': [], '2023-03-30T01:43:44.675000': [], '2023-03-30T01:43:59.645000': [], \n",
    "    '2023-03-30T01:44:16.047000': [], '2023-03-30T01:45:38.975000': [], '2023-03-30T01:45:55.040000': [-3,-2,-1], '2023-03-30T01:47:31.482000': [-1], \n",
    "    '2023-03-30T01:49:14.488000': [], '2023-03-30T01:53:39.161000': [-10000], '2023-03-30T01:55:03.401000': [0,1,2,3,4,5,6,7,8], '2023-03-30T01:55:17.966000': [], \n",
    "    '2023-03-30T01:57:21.863000': [-1], '2023-03-30T01:59:07.979000': [-5000], '2023-03-30T02:14:11.643000': [-10000], '2023-03-30T02:15:48.603000': [-10000], \n",
    "    '2023-03-30T02:18:01.084000': [-10000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dfs = get_sequences(df, interval=5, per_camera=False)\n",
    "print(len(seq_dfs), 'sequences')\n",
    "\n",
    "avg_dust_ratio_and_delta_t = {}\n",
    "\n",
    "for i, seq_df in enumerate(seq_dfs):\n",
    "    if 'total_averaged_dust_ratio' not in seq_df.columns:\n",
    "        # idk where these columns are\n",
    "        seq_df['total_averaged_dust_ratio'] = seq_df.pred_dust_ratio\n",
    "        seq_df['triangle_averaged_dust_ratio'] = seq_df.pred_dust_ratio\n",
    "    fp_df = seq_df[(seq_df.state == 'true_positive') | (seq_df.state == 'false_negative')]\n",
    "    fp_cnt = len(fp_df)\n",
    "    dust_cnt = len(seq_df[seq_df.total_averaged_dust_ratio >= 0.15])\n",
    "    if fp_cnt > 0 and dust_cnt > 0:\n",
    "        # print(i+1, seq_df.iloc[0].datehm, seq_df.iloc[-1].datehm, seq_df.iloc[0].camera_location, fp_cnt, 'FPs in', len(seq_df), 'images, dusty image count', dust_cnt)\n",
    "\n",
    "        # refine points\n",
    "        name = seq_df.iloc[0].collected_on  # use time of original sequence to saved image name\n",
    "        # if not name in fix:\n",
    "        #     continue\n",
    "        if name in fix:\n",
    "            notes = fix[name]\n",
    "            if notes:\n",
    "                if notes[0] == -10000:\n",
    "                    continue\n",
    "                elif notes[0] == -5000:\n",
    "                    fp_df = fp_df.head(n=len(fp_df)//2)\n",
    "                else:\n",
    "                    pos = [idx for idx in notes if idx >= 0]\n",
    "                    neg = [idx for idx in notes if idx < 0]\n",
    "                    if pos:\n",
    "                        fp_df = fp_df.iloc[pos[-1]+1:]\n",
    "                    if neg:\n",
    "                        fp_df = fp_df.iloc[:neg[0]]\n",
    "\n",
    "        # plot individual sequnces\n",
    "        # try:\n",
    "        #     save_path = os.path.join(save_dir, name+'.png')\n",
    "        #     plot_dust_ratio_and_state_rear_pod(fp_df, save_path, plot_in_one=true, only_positive=true, labeled_states=true)\n",
    "        # except:\n",
    "        #     print(i+1, name)\n",
    "        \n",
    "        # process and get average dust ratios and delta_ts\n",
    "        tp_states = get_avg_dust_ratio_and_delta_t(fp_df)\n",
    "        avg_dust_ratio_and_delta_t[name] = [tp_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14014a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(avg_dust_ratio_and_delta_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dust_ratio_and_delta_t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into Day, Dusk, Night\n",
    "day_dusk_split, dusk_night_split = '2023-03-29T23:00:00.000000', '2023-03-30T01:00:00.000000'\n",
    "day_dict = {k:v for k,v in avg_dust_ratio_and_delta_t.items() if k <= day_dusk_split}\n",
    "dusk_dict = {k:v for k,v in avg_dust_ratio_and_delta_t.items() if day_dusk_split < k <= dusk_night_split}\n",
    "night_dict = {k:v for k,v in avg_dust_ratio_and_delta_t.items() if k > dusk_night_split}\n",
    "day_dict.keys(), dusk_dict.keys(), night_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f9d76",
   "metadata": {},
   "source": [
    "## Average dust ratio vs. average/maximum delta_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dicts = {'Day': day_dict, 'Dusk': dusk_dict, 'Night': night_dict}\n",
    "i_list = [0,]\n",
    "i = 0  # 0 for TPs, 1 for TPs and FNs\n",
    "i_states = ['TP', 'TPFN']\n",
    "\n",
    "xrange = {'Day':5, 'Dusk':40, 'Night':15}\n",
    "\n",
    "fig, axarr = plt.subplots(2, 3, figsize=(21, 12))\n",
    "\n",
    "def get_stats(window, partial_dict, keys=['avg_dust_ratio', 'avg_delta_t', 'max_delta_t']):\n",
    "    return_dict = {}\n",
    "    for key in ['avg_dust_ratio', 'avg_delta_t', 'max_delta_t']:\n",
    "        out_list = []\n",
    "        for camera in cameras:\n",
    "            out_list.extend([states_list[i][f'{key}_{camera}{window}'][0] for collected_on, states_list in partial_dict.items() if states_list[i][f'{key}_{camera}{window}']])\n",
    "        if isinstance(out_list[0], list):\n",
    "            out_list = [item for sublist in out_list for item in sublist]\n",
    "        return_dict[key] = out_list\n",
    "    return return_dict\n",
    "\n",
    "for ind, (t, partial_dict) in enumerate(states_dicts.items()):\n",
    "    print(ind)\n",
    "    ax = axarr[0][ind]\n",
    "    print(t, 'Average dust ratio vs. average delta_t')\n",
    "    # plt.figure(1, figsize=(8, 4))\n",
    "    for window in windows:\n",
    "        stats = get_stats(window, partial_dict)\n",
    "        if window == '':\n",
    "            ax.scatter(stats['avg_delta_t'], stats['avg_dust_ratio'], c='red', label='entire_sequence')\n",
    "        else:\n",
    "            ax.scatter(stats['avg_delta_t'], stats['avg_dust_ratio'], label='window_in'+window)\n",
    "    ax.set_xlabel(f'Avg delta_t between two {i_states[i]}s (s)', fontsize=15)\n",
    "    ax.set_ylabel('Avg dust ratio', fontsize=15)\n",
    "    ax.set_title(f'Average dust ratio vs. average delta_t {t}', fontsize=15)\n",
    "    ax.set_xlim([0,xrange[t]])\n",
    "    ax.set_ylim([0,0.5])\n",
    "    ax.legend()\n",
    "    # plt.show()\n",
    "#     plt.savefig(os.path.join(plot_dir, plot_name+'__avg_dust_ratio_vs_avg_delta_t'), transparent=False)\n",
    "#     plt.close()\n",
    "\n",
    "for ind, (t, partial_dict) in enumerate(states_dicts.items()):\n",
    "    ax = axarr[1][ind]\n",
    "    print(t, 'Average dust ratio vs. maximum delta_t')\n",
    "    # plt.figure(1, figsize=(8, 4))\n",
    "    for window in windows:\n",
    "        stats = get_stats(window, partial_dict)\n",
    "        if window == '':\n",
    "            ax.scatter(stats['max_delta_t'], stats['avg_dust_ratio'], c='red', label='entire_sequence')\n",
    "        else:\n",
    "            ax.scatter(stats['max_delta_t'], stats['avg_dust_ratio'], label='window_in' + window)\n",
    "    ax.set_title(f'Average dust ratio vs. maximum delta_t {t}', fontsize=15)\n",
    "    ax.set_xlabel(f'Max delta_t between two {i_states[i]}s (s)', fontsize=15)\n",
    "    ax.set_ylabel('Avg dust ratio', fontsize=15)\n",
    "    ax.set_xlim([0,xrange[t]])\n",
    "    ax.set_ylim([0,0.5])\n",
    "    ax.legend()\n",
    "    # plt.show()\n",
    "#     plt.savefig(os.path.join(plot_dir, plot_name+'__avg_dust_ratio_vs_max_delta_t'), transparent=False)\n",
    "#     plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77642d8",
   "metadata": {},
   "source": [
    "## Average/maximum delta_t histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(2, 3, figsize=(21, 12))\n",
    "\n",
    "i = 0\n",
    "camera = 'rear_side'\n",
    "window = '_5s'  # '', '_5s', '_10s'\n",
    "window_max = 7.0\n",
    "\n",
    "for ind, (t, partial_dict) in enumerate(states_dicts.items()):\n",
    "    print(t, window)\n",
    "    plot_name = t+'__' + ('TP__' if i == 0 else 'TPFN__') + ('max_rear_and_side__' if camera == 'rear_side' else camera+'_') + window\n",
    "    stats = get_stats(window, partial_dict, ['max_delta_t', 'avg_delta_t'])\n",
    "\n",
    "    bins = [0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0,window_max]\n",
    "    print(max(stats['max_delta_t']))\n",
    "# title = 'Average delta_t distribution'\n",
    "#     xlabel = 'Avg delta_t'\n",
    "#     plot_histogram_and_cumulative(stats['avg_delta_t'], bins, title, xlabel, window_max, axarr[0][ind])\n",
    "\n",
    "#     title = 'Maximum delta_t distribution'\n",
    "#     xlabel = 'Max delta_t'\n",
    "#     plot_histogram_and_cumulative(stats['max_delta_t'], bins, title, xlabel, window_max, axarr[1][ind])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e33ca",
   "metadata": {},
   "source": [
    "## Miscellaneous Plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86b0e8",
   "metadata": {},
   "source": [
    "## Plot dust ratios and states (TP,TN,FP,FN) in image sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a722a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dust_ratio_and_state(seq_df, save_path, title='', labeled_states=False, plot_entire_image=True, plot_masked_region=False):\n",
    "    seq_df['datetime'] -= seq_df.iloc[0]['datetime']\n",
    "    plt.figure(1, figsize=(20, 5))\n",
    "    if plot_entire_image and plot_masked_region:\n",
    "        plt.subplot(121)\n",
    "    states = {'true_negative': ['TN', 'blue'], 'false_positive': ['FP', 'orange']}\n",
    "    if labeled_states:\n",
    "        states['false_negative'] = ['FN', 'red']\n",
    "        states['true_positive'] = ['TP', 'green']\n",
    "    if plot_entire_image:\n",
    "        for state, [label, color] in states.items():\n",
    "            sub_df = seq_df[seq_df.state == state]\n",
    "            if len(sub_df) > 0:\n",
    "                plt.scatter(sub_df.datetime.apply(lambda x: x.total_seconds()), sub_df.total_averaged_dust_ratio, label=label, c=color)\n",
    "        plt.title(seq_df.iloc[0].collected_on if len(title) == 0 else title, fontsize=15)\n",
    "        plt.xlabel('Time in sequence (s)', fontsize=15)\n",
    "        plt.ylabel('Dust ratio in entire image', fontsize=15)\n",
    "        plt.legend()\n",
    "    if plot_entire_image and plot_masked_region:\n",
    "        plt.subplot(122)\n",
    "    if plot_masked_region:\n",
    "        for state, [label, color] in states.items():\n",
    "            sub_df = seq_df[seq_df.state == state]\n",
    "            if len(sub_df) > 0:\n",
    "                plt.scatter(sub_df.datetime.apply(lambda x: x.total_seconds()), sub_df.triangle_averaged_dust_ratio, label=label, c=color)\n",
    "        plt.title(seq_df.iloc[0].collected_on if len(title) == 0 else title, fontsize=15)\n",
    "        plt.xlabel('Time in sequence (s)', fontsize=15)\n",
    "        plt.ylabel('Dust ratio in triangles', fontsize=15)\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "#     plt.savefig(save_path)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rear camera, we care more about dust ratios in triangles, so set plot_entire_image=False, plot_masked_region=True\n",
    "rear_df = df[df.camera_location == 'rear-left']\n",
    "plot_dust_ratio_and_state(rear_df, save_path='', title='Rear camera', labeled_states=True, plot_entire_image=False, plot_masked_region=True)\n",
    "# plot side camera, we care more about dust ratios in entire image, so set plot_entire_image=True, plot_masked_region=False\n",
    "side_df = df[df.camera_location.str.startswith('side')]\n",
    "plot_dust_ratio_and_state(side_df, save_path='', title='Side camera', labeled_states=True, plot_entire_image=True, plot_masked_region=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec583ad",
   "metadata": {},
   "source": [
    "### Sometimes we want to plot rear and side cameras in one plot\n",
    " - Note: it could be messy if df contains multiple runs (multiple sequences of vehicles/humans passing tractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa54dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dust_ratio_and_state_rear_pod(seq_df, save_path, plot_in_one=False, only_gt_positive=False, labeled_states=False):\n",
    "    def plot_one_camera(sub_df, states, title, xlabel=False, ylabel_key='', legend_key=''):\n",
    "        for state, [label, color] in states.items():\n",
    "            sub_state_df = sub_df[sub_df.state == state]\n",
    "            if len(sub_state_df) > 0:\n",
    "                marker = \".\"\n",
    "                dust_key = 'triangle_averaged_dust_ratio'\n",
    "                if legend_key.startswith('Side left'):\n",
    "                    marker = \"x\"\n",
    "                    dust_key = 'total_averaged_dust_ratio'\n",
    "                elif legend_key.startswith('Side right'):\n",
    "                    marker = \"+\"\n",
    "                    dust_key = 'total_averaged_dust_ratio'\n",
    "                plt.scatter(sub_state_df.datetime.apply(lambda x: x.total_seconds()), sub_state_df[dust_key], label=legend_key+label, s=100, c=color, marker=marker)\n",
    "        plt.title(sub_df.iloc[0].collected_on if len(title) == 0 else title, fontsize=15)\n",
    "        plt.ylim([-0.02,0.52])\n",
    "        plt.grid(True, axis='y')\n",
    "        if xlabel:\n",
    "            plt.xlabel('Time in sequence (s)', fontsize=15)\n",
    "        plt.ylabel(f'Dust ratio{ylabel_key}', fontsize=15)\n",
    "        plt.legend()\n",
    "    \n",
    "    if only_gt_positive:\n",
    "        seq_df = seq_df[(seq_df.state == 'false_negative') | (seq_df.state == 'true_positive')]\n",
    "    \n",
    "    seq_df['datetime'] -= seq_df.iloc[0]['datetime']\n",
    "    states = {'true_negative': ['TN', 'blue'], 'false_positive': ['FP', 'orange']}\n",
    "    if labeled_states:\n",
    "        states['false_negative'] = ['FN', 'red']\n",
    "        states['true_positive'] = ['TP', 'green']\n",
    "    \n",
    "    side_left_df = seq_df[seq_df.camera_location == 'side-left-left']\n",
    "    rear_df = seq_df[seq_df.camera_location == 'rear-left']\n",
    "    side_right_df = seq_df[seq_df.camera_location == 'side-right-left']\n",
    "    if plot_in_one:\n",
    "#         title = f'{seq_df.iloc[0].collected_on} Rear'\n",
    "        title = 'Rear'\n",
    "        plt.figure(1, figsize=(20, 5))\n",
    "        if len(side_left_df[(side_left_df.state == 'false_negative') | (side_left_df.state == 'true_positive')]) > 0:\n",
    "            plot_one_camera(side_left_df, states, '', False, '', 'Side left ')\n",
    "            title += ' with Side left'\n",
    "        if len(side_right_df[(side_right_df.state == 'false_negative') | (side_right_df.state == 'true_positive')]) > 0:\n",
    "            plot_one_camera(side_right_df, states, '', False, '', 'Side right ')\n",
    "            title += ' with Side right'\n",
    "        plot_one_camera(rear_df, states, title, True, '', 'Rear ')\n",
    "    else:\n",
    "        plt.figure(1, figsize=(20, 15))\n",
    "        plt.subplot(311)\n",
    "        plot_one_camera(side_left_df, states, 'Side-left', False, ' in entire image', 'Side left ')\n",
    "        plt.subplot(312)\n",
    "        plot_one_camera(rear_df, states, 'Rear', False, ' in triangles', 'Rear ')\n",
    "        plt.subplot(313)\n",
    "        plot_one_camera(side_right_df, states, 'Side-right', True, ' in entire image', 'Side right ')\n",
    "    plt.show()\n",
    "#     plt.savefig(save_path, transparent=False)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42802eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dust_ratio_and_state_rear_pod(df, save_path='', plot_in_one=True, only_gt_positive=True, labeled_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef8d07",
   "metadata": {},
   "source": [
    "## Plot states vs. dust ratio for front pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpdf = pdf[(pdf.camera_location.str.startswith('front')) & (pdf.Vehicles > 0)]\n",
    "print(fpdf.shape, fpdf[fpdf.state == 'true_positive'].shape)\n",
    "plt.figure(1, figsize=(20, 3))\n",
    "# plt.scatter(fpdf.pred_dust_ratio, fpdf.state)  # 7-class model dust head\n",
    "plt.scatter(fpdf.total_averaged_dust_ratio, fpdf.state)  # 8 or 4 class model dust class\n",
    "plt.xticks([x*0.01 for x in range(1, 11)])\n",
    "plt.xlabel('Dust ratio', fontsize=15)\n",
    "plt.ylabel('Predicted state', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpdf[fpdf.id == '64429249cd141369ec143c67'].camera_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdef934",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpdf[fpdf.pred_dust_ratio > 0.01].id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ec643",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_on = df2[df2.id == '64429249cd141369ec143c67'].iloc[0].collected_on\n",
    "collected_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[(df2.camera_location == 'side-right-left') & (df2.collected_on >= '2023-04-05T17:10:09.069000') & (df2.collected_on <= '2023-04-05T17:10:09.969000')][['id', 'collected_on']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[pdf.id == '6442d63c94903883d9ef5432'].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8744f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pdf['speed'] < 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpdf = pdf[(pdf.Humans > 0)]\n",
    "tps = fpdf[fpdf.state == 'true_positive'].shape[0]\n",
    "fns = fpdf.shape[0] - tps\n",
    "print(fpdf.shape, fpdf[fpdf.state == 'true_positive'].shape)\n",
    "plt.figure(1, figsize=(20, 3))\n",
    "# plt.scatter(fpdf.pred_dust_ratio, fpdf.state)  # 7-class model dust head\n",
    "plt.scatter(fpdf.total_averaged_dust_ratio, fpdf.state)  # 8 or 4 class model dust class\n",
    "plt.xticks([x*0.025 for x in range(1, 21)])\n",
    "plt.xlabel('Dust ratio', fontsize=15)\n",
    "plt.ylabel('Predicted state', fontsize=15)\n",
    "plt.title(f'Predictions when humans are in the image. tp={tps}, fn={fns}', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb9259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "1eceddbeeb55f686303d64ef8e05e300429be7c506c9f9cad24a6dfe5f27b555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
