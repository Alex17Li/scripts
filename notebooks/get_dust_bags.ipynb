{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "import datetime\n",
    "import io\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from brtdevkit.core.db.athena import AthenaClient\n",
    "from brtdevkit.data import Dataset\n",
    "from timezonefinder import TimezoneFinderL\n",
    "import pytz\n",
    "import cv2\n",
    "from brtdevkit.util.aws.s3 import S3\n",
    "client = S3()\n",
    "\n",
    "from aletheia_dataset_creator.dataset_tools.aletheia_dataset_helpers import imageids_to_dataset\n",
    "from aletheia_dataset_creator.config.dataset_config import LEFT_CAMERAS, ALL_CAMERA_PAIRS_LIST\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "athena = AthenaClient()\n",
    "s3 = boto3.resource('s3')\n",
    "tf = TimezoneFinderL()\n",
    "from pathlib import Path\n",
    "home = Path(os.path.expanduser('~'))\n",
    "data_path = home / 'data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(data_path / 'df_sequences.parquet'):\n",
    "    df_sequences = pd.read_parquet(data_path / 'df_sequences.parquet')\n",
    "else:\n",
    "    query = \"\"\"SELECT ij.id, hard_drive_name, robot_name, collected_on,\n",
    "        bag_name, operating_field_name, operation_time, latitude, longitude, geohash, camera_location, \n",
    "        bundle, group_id, s3_bucket, s3_key, special_notes, image_artifact_jupiter.kind\n",
    "    FROM image_jupiter AS ij\n",
    "    JOIN \"image_artifact_jupiter\" ON ij.\"id\" = \"image_artifact_jupiter\".\"image\"\n",
    "    WHERE \"hard_drive_name\" IN ('JUPD-004_2023-7-19', 'JUPD-006_2023-7-19', 'JUPD-007_2023-7-11') AND image_artifact_jupiter.kind = 'debayeredrgb' AND camera_location IN ('rear-left', 'side-left-left', 'side-right-left')\n",
    "    \"\"\"\n",
    "    df_sequences: pd.DataFrame = athena.get_df(query) # type: ignore\n",
    "    df_sequences.to_parquet(data_path / 'df_sequences.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(data_path / 'df_dusty.parquet'):\n",
    "    df_dusty = pd.read_parquet(data_path / 'df_dusty.parquet')\n",
    "else:\n",
    "    query = \"\"\"SELECT ij.id, hard_drive_name, robot_name, collected_on,\n",
    "        bag_name, operating_field_name, operation_time, latitude, longitude, geohash, camera_location, \n",
    "        bundle, group_id, s3_bucket, s3_key\n",
    "    FROM image_jupiter AS ij\n",
    "    JOIN \"image_artifact_jupiter\" ON ij.\"id\" = \"image_artifact_jupiter\".\"image\"\n",
    "    WHERE \"hard_drive_name\" IN ('JUPD-153_2023-6-29') AND image_artifact_jupiter.kind = 'debayeredrgb' AND camera_location LIKE '%left'\n",
    "    \"\"\"\n",
    "    df_dusty: pd.DataFrame = athena.get_df(query) # type: ignore\n",
    "    df_dusty.to_parquet(data_path / 'df_dusty.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look through sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Hashable\n",
    "df_groups_orig: dict[Hashable, list[Hashable]] = df_sequences.groupby('special_notes').groups\n",
    "df_index_orig = set(df_groups_orig.keys())\n",
    "for e in [\n",
    "    'vehicle in dust time dawn/dusk',\n",
    "    '6508 IQ-test-1',\n",
    "    'vehicle in dust day time ',\n",
    "    'vehicle in dust Day',\n",
    "    '6524 IQ-Test-1',\n",
    "    '6524 IQ-Test-2',\n",
    "    'IQ-image to bright',\n",
    "    # 'Morning dust right side',\n",
    "    # 'Morning dust right side and oil rig',\n",
    "    # 'dust',\n",
    "    'dust right side',\n",
    "    # 'oil rig',\n",
    "    'vehicle dust dusk',\n",
    "]:\n",
    "    df_index_orig.remove(e)\n",
    "df_sequences_valid = df_sequences[df_sequences['special_notes'].isin(df_index_orig)]\n",
    "# rebuild the index and groups\n",
    "df_groups = df_sequences_valid.groupby('special_notes').groups\n",
    "df_index = set(df_groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(df_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(df_index):\n",
    "    folder_name = Path(data_path) / k.replace(' ', '_')\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    for ind in df_groups[k]:\n",
    "        df_row = df_sequences_valid.loc[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(df_row, collected_on: str):\n",
    "    if len(df_row) == 0:\n",
    "        whiteFrame = 255 * np.ones((604, 964, 3), np.uint8)\n",
    "        font = cv2.FONT_HERSHEY_PLAIN\n",
    "        whiteFrame = cv2.putText(whiteFrame, collected_on, (50, 400), font, 5, (0,0,0), 5)\n",
    "        return whiteFrame\n",
    "    elif isinstance(df_row, pd.DataFrame):\n",
    "        assert len(df_row) == 1\n",
    "        df_row = df_row.iloc[0]\n",
    "    file_name = Path(data_path) / df_row['special_notes'].replace(' ', '_') / str(df_row.id + '.png')\n",
    "    if not os.path.exists(file_name):\n",
    "        client.download_file(df_row['s3_bucket'], df_row['s3_key'], file_name)\n",
    "    im = cv2.imread(str(file_name))\n",
    "    return im\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_frames(k: str):\n",
    "    \"\"\"\n",
    "    Given dictionary with image paths creates concatenated image and video and saves to output_dir.\n",
    "    :param grouped_images: List with a dictionary per group_id\n",
    "    :param bag_or_drive_name: AnyStr hard_drive_name or bag_name given during data ingestion\n",
    "    :param output_dir: AnyStr path to save the video directory\n",
    "    \"\"\"\n",
    "    video_dir = Path(data_path) / 'videos' / f\"{k.replace(' ', '_')}\"\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    video_name = video_dir / \"video.mp4\"\n",
    "    if os.path.exists(video_name):\n",
    "        return\n",
    "    writer = imageio.get_writer(video_name, fps=1)\n",
    "    k_df = df_sequences.loc[df_groups[k]].sort_values('collected_on')\n",
    "    k_groups = k_df.groupby('group_id').groups\n",
    "    seen = set()\n",
    "    for row in tqdm(k_df.iterrows()):\n",
    "        gid = row[1]['group_id']\n",
    "        if gid in seen:\n",
    "            continue\n",
    "        seen.add(gid)\n",
    "        values = k_groups[gid]\n",
    "        group = df_sequences.loc[values]\n",
    "        collected_on_str = str(group.iloc[0].collected_on)[11:19]\n",
    "        # try:\n",
    "        # concatenate image Horizontally\n",
    "        front_pod = np.concatenate(\n",
    "            (\n",
    "                get_image(group[group['camera_location'] == 'front-left-left'], collected_on_str),\n",
    "                get_image(group[group['camera_location'] == 'front-center-left'], collected_on_str),\n",
    "                get_image(group[group['camera_location'] == 'front-right-left'], collected_on_str),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        rear_pod = np.concatenate(\n",
    "            (\n",
    "                get_image(group[group['camera_location'] == 'side-left-left'], collected_on_str),\n",
    "                get_image(group[group['camera_location'] == 'rear-left'], collected_on_str),\n",
    "                get_image(group[group['camera_location'] == 'side-right-left'], collected_on_str),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        # concatenate image vertically\n",
    "        all_cameras = np.concatenate((front_pod, rear_pod), axis=0)[::4, ::4, ::-1]\n",
    "        # save concatenated image file\n",
    "        full_img_name = f\"{collected_on_str}.png\"\n",
    "        file_path = os.path.join(video_dir, full_img_name)\n",
    "        plt.imsave(file_path, all_cameras)\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        writer.append_data(imageio.imread(file_path))\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Skipping frame. Exception occurred: {e}\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle in dust dusk 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 742/1548 [03:10<03:26,  3.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m         file_name \u001b[39m=\u001b[39m folder_name \u001b[39m/\u001b[39m \u001b[39mstr\u001b[39m(df_row\u001b[39m.\u001b[39mid \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(file_name):\n\u001b[0;32m---> 11\u001b[0m             client\u001b[39m.\u001b[39;49mdownload_file(df_row[\u001b[39m'\u001b[39;49m\u001b[39ms3_bucket\u001b[39;49m\u001b[39m'\u001b[39;49m], df_row[\u001b[39m'\u001b[39;49m\u001b[39ms3_key\u001b[39;49m\u001b[39m'\u001b[39;49m], file_name)\n\u001b[1;32m     12\u001b[0m \u001b[39m# 2) Make all of the videos\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m tqdm(df_index):\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/brtdevkit/util/aws/s3.py:101\u001b[0m, in \u001b[0;36mS3.download_file\u001b[0;34m(self, bucket_name, key, target_path)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Download a file object from an S3 bucket to a local target path.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m    bool - Whether the download was successful or not.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mdownload_file(Bucket\u001b[39m=\u001b[39;49mbucket_name, Key\u001b[39m=\u001b[39;49mkey, Filename\u001b[39m=\u001b[39;49mtarget_path)\n\u001b[1;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mexcept\u001b[39;00m botocore\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mClientError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/boto3/s3/inject.py:190\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[39mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39m    transfer.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39mwith\u001b[39;00m S3Transfer(\u001b[39mself\u001b[39m, Config) \u001b[39mas\u001b[39;00m transfer:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m transfer\u001b[39m.\u001b[39;49mdownload_file(\n\u001b[1;32m    191\u001b[0m         bucket\u001b[39m=\u001b[39;49mBucket,\n\u001b[1;32m    192\u001b[0m         key\u001b[39m=\u001b[39;49mKey,\n\u001b[1;32m    193\u001b[0m         filename\u001b[39m=\u001b[39;49mFilename,\n\u001b[1;32m    194\u001b[0m         extra_args\u001b[39m=\u001b[39;49mExtraArgs,\n\u001b[1;32m    195\u001b[0m         callback\u001b[39m=\u001b[39;49mCallback,\n\u001b[1;32m    196\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/boto3/s3/transfer.py:326\u001b[0m, in \u001b[0;36mS3Transfer.download_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    322\u001b[0m future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_manager\u001b[39m.\u001b[39mdownload(\n\u001b[1;32m    323\u001b[0m     bucket, key, filename, extra_args, subscribers\n\u001b[1;32m    324\u001b[0m )\n\u001b[1;32m    325\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m     future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    327\u001b[0m \u001b[39m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m# exceeded we need to throw the same error from boto3 instead of\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m# s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39m# catching the boto3 one instead of the s3transfer exception to do\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m# their own retries.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39mexcept\u001b[39;00m S3TransferRetriesExceededError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/s3transfer/futures.py:106\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel()\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/s3transfer/futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[39m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[39m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[39m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_coordinator\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    104\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/site-packages/s3transfer/futures.py:261\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Waits until TransferFuture is done and returns the result\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[39mIf the TransferFuture succeeded, it will return the result. If the\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mTransferFuture failed, it will raise the exception associated to the\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39mfailure.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# Doing a wait() with no timeout cannot be interrupted in python2 but\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# can be interrupted in python3 so we just wait with the largest\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m# possible value integer value, which is on the scale of billions of\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39m# years...\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_done_event\u001b[39m.\u001b[39;49mwait(MAXINT)\n\u001b[1;32m    263\u001b[0m \u001b[39m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# final result.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 600\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    601\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/cvml/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1) Download all of the images\n",
    "from tqdm import tqdm\n",
    "for i, k in enumerate(df_index):\n",
    "    print(k)\n",
    "    folder_name = Path(data_path) / k.replace(' ', '_')\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    for ind in tqdm(df_groups[k]):\n",
    "        df_row = df_sequences.loc[ind]\n",
    "        file_name = folder_name / str(df_row.id + '.png')\n",
    "        if not os.path.exists(file_name):\n",
    "            client.download_file(df_row['s3_bucket'], df_row['s3_key'], file_name)\n",
    "# 2) Make all of the videos\n",
    "for k in tqdm(df_index):\n",
    "    create_video_frames(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "def get_start_offset_fromtime(movie, hour, minute, second):\n",
    "    timestamp = movie.iloc[0]['collected_on']\n",
    "    y, m, d = timestamp.year, timestamp.month, timestamp.day\n",
    "    start_t = datetime(y, m, d, hour, minute, second, tzinfo=movie['collected_on'].iloc[0].tzinfo)\n",
    "    return (movie['collected_on'] > start_t).argmax()\n",
    "img_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179260"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = list(df_index)[0]\n",
    "# for i in range(0, 10):\n",
    "#     df_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'im_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client\u001b[39m.\u001b[39mdownload_file(df_row[\u001b[39m'\u001b[39m\u001b[39ms3_bucket\u001b[39m\u001b[39m'\u001b[39m], df_row[\u001b[39m'\u001b[39m\u001b[39ms3_key\u001b[39m\u001b[39m'\u001b[39m], im_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'im_path' is not defined"
     ]
    }
   ],
   "source": [
    "client.download_file(df_row['s3_bucket'], df_row['s3_key'], im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "def plot(movie_name, movie=None, ncols=20, offset=0, step=1) -> None:\n",
    "    offset = max(0, min(offset, len(movie) - (ncols - 1) * step))\n",
    "    print(f\"Watching \\\"{movie_name}\\\". Total images {len(movie)}, viewing {ncols - 1} starting from {offset} with step {step}\")\n",
    "    rows_per_cat = (4 + ncols) // 5\n",
    "    nrows = rows_per_cat\n",
    "    fig, ax = plt.subplots(nrows, 5, figsize=(16, nrows * 4))\n",
    "\n",
    "    ax0 = ax[0][0]\n",
    "    ax0.set_title(movie_name)\n",
    "    tz = tf.timezone_at(lng=movie.iloc[0]['longitude'], lat=movie.iloc[0]['latitude'])\n",
    "    ax0.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M', tz=tz))\n",
    "    ax0.xaxis.set_major_locator(mdates.HourLocator(interval=3, tz=tz))\n",
    "    for j in range(1, ncols):\n",
    "        idx = offset + j * step - 1\n",
    "        if idx >= len(movie):\n",
    "            break\n",
    "        df_row = movie.iloc[idx]\n",
    "        im_path = Path(data_path) / str(movie_name.replace(' ', '_')) / (str(df_row['id']) + '.png')\n",
    "        if im_path not in img_cache:\n",
    "            client.download_file(df_row['s3_bucket'], df_row['s3_key'], im_path)\n",
    "            img_cache[im_path] = cv2.imread(str(im_path))\n",
    "        im = img_cache[im_path]\n",
    "        ax[j // 5][j % 5].imshow(im[::2,::2, ::-1])\n",
    "        ax[j // 5][j % 5].set_title(f\"{str(df_row['collected_on'])[11:19]}\")\n",
    "k = list(df_index)[0]\n",
    "movie = df_sequences.iloc[df_groups[k]].sort_values('collected_on')\n",
    "step = 1\n",
    "ncols = 10\n",
    "offset = get_start_offset_fromtime(movie, hour=1, minute=59, second=25)\n",
    "# offset = 0\n",
    "ncols = len(movie) // step\n",
    "plot(k, movie, ncols, offset, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = {\n",
    "    \"vehicle dust dusk 35\": [\"start\", \"end\"],\n",
    "    \"vehicle dust dusk 37\": [\"start\", \"end\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look through dusty\n",
    "df_dusty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
