{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexli/miniconda3/envs/cvml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "import datetime\n",
    "import io\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from brtdevkit.core.db.athena import AthenaClient\n",
    "from brtdevkit.data import Dataset\n",
    "from timezonefinder import TimezoneFinderL\n",
    "import pytz\n",
    "import cv2\n",
    "from brtdevkit.util.aws.s3 import S3\n",
    "client = S3()\n",
    "\n",
    "from aletheia_dataset_creator.dataset_tools.aletheia_dataset_helpers import imageids_to_dataset\n",
    "from aletheia_dataset_creator.config.dataset_config import LEFT_CAMERAS, ALL_CAMERA_PAIRS_LIST\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "athena = AthenaClient()\n",
    "s3 = boto3.resource('s3')\n",
    "tf = TimezoneFinderL()\n",
    "from pathlib import Path\n",
    "home = Path(os.path.expanduser('~'))\n",
    "data_path = home / 'data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(data_path / 'df_sequences.parquet'):\n",
    "    df_sequences = pd.read_parquet(data_path / 'df_sequences.parquet')\n",
    "else:\n",
    "    print(\"Cache miss\")\n",
    "    query = \"\"\"SELECT ij.id, hard_drive_name, robot_name, collected_on,\n",
    "        bag_name, operating_field_name, operation_time, latitude, longitude, geohash, camera_location, \n",
    "        bundle, group_id, s3_bucket, s3_key, special_notes, image_artifact_jupiter.kind\n",
    "    FROM image_jupiter AS ij\n",
    "    JOIN \"image_artifact_jupiter\" ON ij.\"id\" = \"image_artifact_jupiter\".\"image\"\n",
    "    WHERE \"hard_drive_name\" IN ('JUPD-004_2023-7-19', 'JUPD-006_2023-7-19', 'JUPD-007_2023-7-11') AND image_artifact_jupiter.kind = 'debayeredrgb' AND camera_location IN ('rear-left', 'side-left-left', 'side-right-left')\n",
    "    \"\"\"\n",
    "    df_sequences: pd.DataFrame = athena.get_df(query) # type: ignore\n",
    "    df_sequences.to_parquet(data_path / 'df_sequences.parquet')\n",
    "df_sequences['image_id'] = df_sequences['id']\n",
    "df_sequences = df_sequences.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(data_path / 'df_dusty.parquet'):\n",
    "    df_dusty = pd.read_parquet(data_path / 'df_dusty.parquet')\n",
    "else:\n",
    "    print(\"Cache miss\")\n",
    "    query = \"\"\"SELECT ij.id, hard_drive_name, robot_name, collected_on,\n",
    "        bag_name, operating_field_name, operation_time, latitude, longitude, geohash, camera_location, \n",
    "        bundle, group_id, s3_bucket, s3_key, special_notes\n",
    "    FROM image_jupiter AS ij\n",
    "    JOIN \"image_artifact_jupiter\" ON ij.\"id\" = \"image_artifact_jupiter\".\"image\"\n",
    "    WHERE \"hard_drive_name\" IN ('JUPD-054_2023-6-13') AND image_artifact_jupiter.kind = 'debayeredrgb' AND camera_location LIKE '%left'\n",
    "    \"\"\"\n",
    "    df_dusty: pd.DataFrame = athena.get_df(query) # type: ignore\n",
    "    df_dusty.to_parquet(data_path / 'df_dusty.parquet')\n",
    "df_dusty['image_id'] = df_dusty['id']\n",
    "df_dusty = df_dusty.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VAT_1137_Human_on _implement-post-misuse-with-in-30-min',\n",
       " 'VAT-57-side-left-left&right-cam-full',\n",
       " 'Dust-test-1-pos-5-atmp-2',\n",
       " 'Dust-test-1-pos-1-atmp-2',\n",
       " 'VAT_1137_Human_on _implement-post-misuse-with-in-30-min-stationary',\n",
       " 'VAT-57-side-left-right-cam-full',\n",
       " 'ISO-LO-TEST-56cm',\n",
       " 'Dynamic-Vehicle-&Stationary-Human-FL ',\n",
       " 'ISO-LO-TEST-53cm-9.5mph',\n",
       " 'Dust-test-1-pos-1',\n",
       " 'VAT-57-rear-right&left-cam-full',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-2fps-auto-1',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-1.5fps-stationary-2',\n",
       " 'Dynamic-Vehicle-&Dynamic-Human from FC tractor9.5m-2[right-wheel]ph]',\n",
       " 'VAT_1147_>4mph-from-rear-right-2m-1',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-1fps-stationary',\n",
       " 'VAT_1147_>4mph-from-rear-leftt-2m-1',\n",
       " 'VAT-57-side-right-left-cam-partial',\n",
       " 'AprilTag_30ft_VT',\n",
       " 'VAT_1147_>4mph-from-rear-right-2m-2',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-4fps-auto',\n",
       " 'Dynamic-Vehicle-&Dynamic-Human from FC tractor9.5m-2[impl-right-wheel]',\n",
       " 'VAT-57-side-right-left-cam-full',\n",
       " 'DA-800ms',\n",
       " 'project-tri-force-vpu3-DA-400-ui-3fps-stationary',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-1.5fps-stationary',\n",
       " 'VAT-57-side-left-left&right-cam-partial',\n",
       " 'Stationary-Vehicle-&-Dynamic-Human-FL--4',\n",
       " 'ArUco_30ft_VT',\n",
       " 'project-tri-force-vpu3-DA-800-ui-3fps-stationary',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-2fps-auto-2',\n",
       " 'VAT-57-side-left-right-cam-partial',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-3fps-auto',\n",
       " 'project-tri-force-vpu3-DA-800-ui-3fps-auto',\n",
       " 'Stationary-Vehicle-&-Dynamic-Human-FR-IBZ-2',\n",
       " 'VAT_1147_>4mph-from-rear-leftt-2m-2',\n",
       " 'VAT-57-rear-right-cam-full',\n",
       " 'Dust-test-1-pos-3-atmp-1',\n",
       " 'Dust-test-1-pos-3-atmp-2',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-2fps-stationary',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-3fps-human',\n",
       " 'VAT-57-side-right-right-cam-full',\n",
       " 'Stationary-Vehicle-&-Dynamic-Human-FL-rear-wheel',\n",
       " 'VAT-57-side-left-left-cam-partial',\n",
       " 'project-tri-force-vpu3-DA-600-ui-3fps-stationary',\n",
       " 'Stationary-Vehicle-&-Dynamic-Human-FR-rear-wheel',\n",
       " 'Dynamic-Vehicle-&-Human from side-right of the tractor9.5mph]',\n",
       " 'VAT-57-side-left-left-cam-full',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-4fps-stationary',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-3fps-FIT',\n",
       " 'Dust-test-1-pos-2-atmp-1',\n",
       " 'Stationary-Vehicle-&-Dynamic-Human-FR-front-wheel',\n",
       " 'Dust-test-1-pos-5-atmp-1',\n",
       " 'project-tri-force-vpu3-DA-5000',\n",
       " 'DA-1000ms',\n",
       " 'project-tri-force-vpu3-DA-1000+ui',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-3fps-stationary',\n",
       " 'VAT-57-rear-left-cam-full',\n",
       " 'VAT-57-rear-left--cam-partial',\n",
       " 'VAT-57-rear-right&left-cam-partial',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-2fps-auto',\n",
       " 'project-tri-force-vpu3-DA-1000-ui-2fps-stationary-2',\n",
       " 'VAT-57-side-right-right-cam-partial',\n",
       " 'VAT_1147_>4mph-from-rear-leftt-2m',\n",
       " 'VAT_1147_>4mph-from-rear-right-2m',\n",
       " 'VAT-57-rear-right--cam-partial',\n",
       " 'Dust-test-1-pos-4-atmp-1',\n",
       " 'project-tri-force-vpu3-DA-1000',\n",
       " 'VAT-57-side-right-left&right-cam-partial',\n",
       " 'ISO-LO-TEST-53cm-6mph',\n",
       " <NA>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look through sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Hashable\n",
    "df_groups_orig: dict[Hashable, list[Hashable]] = df_sequences.groupby('special_notes').groups\n",
    "df_index_orig = set(df_groups_orig.keys())\n",
    "merged_runs = []\n",
    "for e in [\n",
    "    'vehicle in dust time dawn/dusk',\n",
    "    'vehicle in dust day time ',\n",
    "    'vehicle in dust Day',\n",
    "    'vehicle dust dusk',\n",
    "]:\n",
    "    merged_runs.append(df_sequences[df_sequences['special_notes'] == e])\n",
    "merged_runs = pd.concat(merged_runs)\n",
    "for e in [\n",
    "    '6508 IQ-test-1',\n",
    "    '6524 IQ-Test-1',\n",
    "    '6524 IQ-Test-2',\n",
    "    'IQ-image to bright',\n",
    "    'dust right side',\n",
    "]:\n",
    "    df_index_orig.remove(e)\n",
    "df_sequences_valid = df_sequences[df_sequences['special_notes'].isin(df_index_orig)].copy()\n",
    "# rebuild the index and groups\n",
    "df_groups = df_sequences_valid.groupby('special_notes').groups\n",
    "df_index = set(df_groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, <NA>}\n"
     ]
    }
   ],
   "source": [
    "def get_run_id(df_row):\n",
    "    try:\n",
    "        return int(df_row['special_notes'].split(\" \")[-1])\n",
    "    except ValueError:\n",
    "        return pd.NA\n",
    "df_sequences_valid['run_id'] = df_sequences_valid.apply(get_run_id, axis=1)\n",
    "print(set(df_sequences_valid['run_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "interval = 5\n",
    "\n",
    "known_ids = set(df_sequences_valid['run_id'])\n",
    "total_sequences = 0\n",
    "merged_runs = merged_runs.sort_values('collected_on')\n",
    "# merged_runs['c'] = merged_runs.collected_on.apply(datetime.fromisoformat)\n",
    "delta = timedelta(seconds=interval)\n",
    "start_t = merged_runs.iloc[0].collected_on\n",
    "for i in range(1, len(merged_runs)):\n",
    "    end_t = merged_runs.iloc[i - 1].collected_on\n",
    "    next_t = merged_runs.iloc[i].collected_on\n",
    "    if next_t - end_t > delta or i == len(merged_runs) - 1:\n",
    "        if i == len(merged_runs) - 1:\n",
    "            next_t += timedelta(microseconds=1)\n",
    "        known_ids.add(total_sequences)\n",
    "        while total_sequences in known_ids:\n",
    "            total_sequences += 1\n",
    "        merged_runs.loc[(start_t <= merged_runs['collected_on']) & (merged_runs['collected_on'] < next_t), 'run_id'] = total_sequences\n",
    "        start_t = next_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for id, row in df_sequences_valid.iterrows():\n",
    "    try:\n",
    "        out.append(merged_runs.loc[id]['run_id'])\n",
    "    except KeyError:\n",
    "        out.append(int(row['special_notes'].split(\" \")[-1]))\n",
    "        \n",
    "df_sequences_valid['run_id'] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "# Should be 72?\n",
    "df_sequences_valid[df_sequences_valid['run_id'].isna()]\n",
    "print(total_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(df_row, collected_on: str, folder_name: str):\n",
    "    if len(df_row) == 0:\n",
    "        whiteFrame = 255 * np.ones((604, 964, 3), np.uint8)\n",
    "        font = cv2.FONT_HERSHEY_PLAIN\n",
    "        whiteFrame = cv2.putText(whiteFrame, collected_on, (50, 400), font, 5, (0,0,0), 5)\n",
    "        return whiteFrame\n",
    "    elif isinstance(df_row, pd.DataFrame):\n",
    "        assert len(df_row) == 1\n",
    "        df_row = df_row.iloc[0]\n",
    "    file_name = Path(data_path) / folder_name / (str(df_row.image_id) + '.png')\n",
    "    if not os.path.exists(file_name):\n",
    "        client.download_file(df_row['s3_bucket'], df_row['s3_key'], file_name)\n",
    "    im = cv2.imread(str(file_name))\n",
    "    return im\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_frames(file_prefix: str, base_df: pd.DataFrame, folder_name: str):\n",
    "    \"\"\"\n",
    "    Given dictionary with image paths creates concatenated image and video and saves to output_dir.\n",
    "    :param grouped_images: List with a dictionary per group_id\n",
    "    :param bag_or_drive_name:AnyStr hard_drive_name or bag_name given during data ingestion\n",
    "    :param output_dir: AnyStr path to save the video directory\n",
    "    \"\"\"\n",
    "    video_dir = Path(data_path) / 'videos' / str(file_prefix) \n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    video_name = video_dir / \"video.mp4\"\n",
    "    if os.path.exists(video_name):\n",
    "        return\n",
    "    writer = imageio.get_writer(video_name, fps=1)\n",
    "    k_df = base_df.sort_values('collected_on')\n",
    "    k_groups = base_df.groupby('group_id').groups\n",
    "    seen = set()\n",
    "    print(len(k_df))\n",
    "    for row in tqdm(k_df.iterrows()):\n",
    "        gid = row[1]['group_id']\n",
    "        if gid in seen:\n",
    "            continue\n",
    "        seen.add(gid)\n",
    "        values = k_groups[gid]\n",
    "        group = k_df.loc[values]\n",
    "        collected_on_str = str(group.iloc[0].collected_on)[11:19]\n",
    "        # try:\n",
    "        # concatenate image Horizontally\n",
    "        front_pod = np.concatenate(\n",
    "            (\n",
    "                get_image(group[group['camera_location'] == 'front-left-left'], collected_on_str, folder_name),\n",
    "                get_image(group[group['camera_location'] == 'front-center-left'], collected_on_str, folder_name),\n",
    "                get_image(group[group['camera_location'] == 'front-right-left'], collected_on_str, folder_name),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        rear_pod = np.concatenate(\n",
    "            (\n",
    "                get_image(group[group['camera_location'] == 'side-left-left'], collected_on_str, folder_name),\n",
    "                get_image(group[group['camera_location'] == 'rear-left'], collected_on_str, folder_name),\n",
    "                get_image(group[group['camera_location'] == 'side-right-left'], collected_on_str, folder_name),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        # concatenate image vertically\n",
    "        all_cameras = np.concatenate((front_pod, rear_pod), axis=0)[::4, ::4, ::-1]\n",
    "        # save concatenated image file\n",
    "        full_img_name = f\"{collected_on_str}.png\"\n",
    "        file_path = os.path.join(video_dir, full_img_name)\n",
    "        plt.imsave(file_path, all_cameras)\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        writer.append_data(imageio.imread(file_path))\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Skipping frame. Exception occurred: {e}\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Download all of the images\n",
    "from tqdm import tqdm\n",
    "for i in range(1, 1 + total_sequences):\n",
    "    folder_name = Path(data_path) / str(int(i))\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    base_df = df_sequences_valid[df_sequences_valid['run_id'] == i]\n",
    "    for id, df_row in base_df.iterrows():\n",
    "        file_name = folder_name / str(id + '.png')\n",
    "        if not os.path.exists(file_name):\n",
    "            client.download_file(df_row['s3_bucket'], df_row['s3_key'], file_name)\n",
    "# 2) Make all of the videos\n",
    "# for i in range(1, 1 + total_sequences):\n",
    "    create_video_frames(str(i), base_df, str(int(df_row['run_id'])))\n",
    "    # print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41545\n"
     ]
    }
   ],
   "source": [
    "print(len(df_sequences_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_orig = df_sequences_valid.groupby('run_id').groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time cutoffs such that the video shows one pass of the vehicle moving forwards.\n",
    "# Remove any time that the vehicle is lingering/ not moving\n",
    "# All found by hand via observation of the downloaded videos\n",
    "from typing import Dict, List, Tuple\n",
    "cutoff_dict: Dict[int, List[Tuple[str, str]]] = {\n",
    "    6: [], # just lingers, then stops\n",
    "    18: [],\n",
    "    28: [],\n",
    "    51: [],\n",
    "    53: [],\n",
    "    56: [],\n",
    "    59: [], # car does not move forward the whole time, stays in dustcloud\n",
    "    72: [], # yo its a human!\n",
    "    85: [], # sequence is a bit cut off at the start\n",
    "\n",
    "    # White SUV\n",
    "    1: [('21:28:50', '21:29:03')],\n",
    "    2: [('21:29:49', '21:30:04')],\n",
    "    3: [('21:31:53', '21:32:16')],\n",
    "    4: [('21:59:20', '21:59:41')], # hard\n",
    "    5: [('22:00:10', '22:00:30')],\n",
    "    7: [('22:15:30', '22:15:55')], # hard, lingers at the start in huge dust\n",
    "    8: [('22:16:10', '22:16:25')],\n",
    "    9: [('22:16:47', '22:17:00'), ('22:17:08', '22:17:25')], # does 2 runs, one each side\n",
    "    10: [('22:21:13', '22:21:29')],\n",
    "    11: [('22:21:47', '22:22:06')],\n",
    "    12: [('22:22:44', '22:23:00')],\n",
    "    13: [('22:23:32', '22:23:41')],\n",
    "    14: [('22:29:12', '22:29:27')],\n",
    "    15: [('22:29:57', '22:30:10')],\n",
    "    16: [('22:30:35', '22:30:54')],\n",
    "    17: [('22:31:38', '22:31:48')],\n",
    "    19: [('11:28:30', '11:28:48')],\n",
    "    20: [('11:33:18', '11:33:33')],\n",
    "    21: [('21:53:00', '21:53:11')],\n",
    "    22: [('21:53:56', '21:54:09')],\n",
    "    23: [('21:55:00','21:55:15'), ('21:55:56', '21:56:07')],\n",
    "    24: [('21:56:56','21:57:08')],\n",
    "    25: [('22:05:29','22:05:40')],\n",
    "    26: [('22:17:52','22:18:00')],\n",
    "    27: [('22:19:07','22:19:19')],\n",
    "    36: [('22:20:22', '22:20:36')],\n",
    "    46: [('22:21:29', '22:21:42')],\n",
    "    47: [('22:27:58', '22:28:07'), ('22:29:00', '22:29:15')],\n",
    "    48: [('22:29:55', '22:30:20')],\n",
    "    49: [('22:35:10', '22:35:30')],\n",
    "    50: [('22:36:17', '22:36:24')],\n",
    "    52: [('22:42:00', '22:42:18'), ('22:43:08', '22:43:25')],\n",
    "    54: [('22:49:04', '22:49:13')],\n",
    "    55: [('22:50:40', '22:50:53')],\n",
    "\n",
    "    29: [('01:40:00', '01:41:13'), ('01:42:02', '01:42:12')],\n",
    "    30: [('01:49:28', '01:49:35')],\n",
    "    31: [('01:50:12', '01:50:22')],\n",
    "    32: [('01:56:52', '01:57:25')], # added some extra frames at the start since the tractor was turning which seems interesting\n",
    "    33: [('01:58:08', '01:58:19')],\n",
    "    34: [('01:58:55', '01:58:03')],\n",
    "    35: [('01:59:35', '01:59:49')],\n",
    "\n",
    "    # Begin black SUV\n",
    "    37: [('03:12:05', '03:12:22')],\n",
    "    38: [('03:13:22', '03:13:35')],\n",
    "    39: [('03:20:20', '03:20:50')],\n",
    "    40: [('03:21:35', '03:21:54')],\n",
    "    41: [('03:22:40', '03:22:54')],\n",
    "    42: [('03:27:35', '03:28:03')],\n",
    "    43: [('03:28:35', '03:28:53')],\n",
    "    44: [('03:29:28', '03:29:40')],\n",
    "    45: [('03:33:49', '03:34:44')],\n",
    "\n",
    "    # white again\n",
    "    57: [('01:01:57','01:02:14'), ('01:02:44','01:03:01')],\n",
    "    58: [('01:04:00','01:04:22'), ('01:04:48','01:05:05')],\n",
    "    60: [('01:06:28', '01:06:43'), ('01:07:16', '01:07:30')],\n",
    "    61: [('01:08:16', '1:08:31')],\n",
    "    62: [('01:19:52', '1:19:59')],\n",
    "    63: [],\n",
    "    64: [('01:22:10', '1:22:17'), ('1:23:34', '1:23:42')],\n",
    "    65: [('01:24:32', '1:24:45')],\n",
    "    66: [('01:25:17', '01:25:27')],\n",
    "    67: [('01:25:59', '01:26:13')],\n",
    "    68: [('01:27:00', '01:27:09')],\n",
    "    69: [('01:27:42', '01:27:56')],\n",
    "    70: [('01:42:32', '01:42:38')],\n",
    "    71: [('01:42:32', '01:42:38')],\n",
    "    # black suv again :)\n",
    "    73: [('01:00:55', '01:01:14')],\n",
    "    74: [('01:02:00', '01:02:14')],\n",
    "    75: [('01:03:00', '01:03:10')],\n",
    "    76: [('01:08:45', '01:08:55')],\n",
    "    77: [('01:09:30', '01:09:45')],\n",
    "    78: [('01:10:30', '01:10:45')],\n",
    "    79: [('01:16:30', '01:16:44')],\n",
    "    80: [('01:17:21', '01:17:34')],\n",
    "    81: [('01:18:08', '01:18:17')],\n",
    "    82: [('01:18:49', '01:18:59')],\n",
    "    83: [('01:24:32', '01:24:50')],\n",
    "    84: [('01:25:15', '01:25:25')],\n",
    "    86: [('01:26:45', '01:26:55')],\n",
    "    87: [('01:32:08', '01:32:26')],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def filter_movie(movie, start_t, end_t):\n",
    "    timestamp = movie.iloc[0]['collected_on']\n",
    "    s_hour, s_minute, s_second = map(int, start_t.split(':'))\n",
    "    e_hour, e_minute, e_second = map(int, end_t.split(':'))\n",
    "    y, m, d = timestamp.year, timestamp.month, timestamp.day\n",
    "    tzinfo = movie['collected_on'].iloc[0].tzinfo\n",
    "    start_dt = datetime(y, m, d, s_hour, s_minute, s_second, tzinfo=tzinfo)\n",
    "    end_dt = datetime(y, m, d, e_hour, e_minute, e_second, tzinfo=tzinfo)\n",
    "    return movie[(start_dt < movie['collected_on']) & (movie['collected_on'] < end_dt)]\n",
    "img_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_runs = []\n",
    "for run_id in range(1, 88):\n",
    "    sequence = df_sequences_valid.loc[runs_orig[run_id]]\n",
    "    times = cutoff_dict[run_id]\n",
    "    for start_t, end_t in cutoff_dict[run_id]:\n",
    "        filtered = filter_movie(sequence, start_t, end_t)\n",
    "        # if not len(filtered):\n",
    "        #     print(run_id, start_t, end_t ) # oops\n",
    "        cleaned_runs.append(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = pd.concat(cleaned_runs, keys=list(range(len(cleaned_runs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(cleaned_df.image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_slow(from_df, name, description) -> None:\n",
    "    imids = list(from_df['image_id'])\n",
    "    desc = f\"{description} ({len(from_df['image_id'])} images)\"\n",
    "    print(len(imids))\n",
    "    from_df.to_parquet(data_path / '{name}.parquet', index=False)\n",
    "    imageids_to_dataset(imids, name, dataset_kind='image',\n",
    "                             dataset_description=desc)\n",
    "# make_dataset_slow(cleaned_df, \"suv_driving_through_rear_dust\", \"87 sequences of rear+rear side data where a (white/black) suv drives through dust, starting from behind the tractor and ending up on the side of it. Collected 2023 July 12-14.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.create(\n",
    "#     name='suv_driving_through_rear_dust_left_cam',\n",
    "#     description=\"87 sequences of rear+rear side data where a (white/black) suv drives through dust, starting from behind the tractor and ending up on the side of it. Collected 2023 July 12-14. Left cameras only (11080 images)\",\n",
    "#     kind=Dataset.KIND_IMAGE,\n",
    "#     image_ids=list(cleaned_df['image_id']),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look through dusty human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import pretty\n",
    "pretty.install()\n",
    "df_dusty = df_dusty[df_dusty['special_notes'].notna()]\n",
    "valid_notes = [\n",
    "    'Dust-test-1-pos-1',\n",
    "    'Dust-test-1-pos-1-atmp-2',\n",
    "    'Dust-test-1-pos-2-atmp-1',\n",
    "    'Dust-test-1-pos-3-atmp-1',\n",
    "    'Dust-test-1-pos-3-atmp-2',\n",
    "    'Dust-test-1-pos-4-atmp-1',\n",
    "    'Dust-test-1-pos-5-atmp-1',\n",
    "    'Dust-test-1-pos-5-atmp-2',\n",
    "]\n",
    "df_dusty = df_dusty[df_dusty['special_notes'].isin(valid_notes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dusty = df_dusty.sort_values('collected_on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dusty.sort_values('collected_on'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dust-test-1-pos-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [00:00<00:00, 26190.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (723, 302) to (736, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "14it [00:00, 38.17it/s][swscaler @ 0x663cf80] Warning: data is not aligned! This can lead to a speed loss\n",
      "564it [00:11, 48.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dust-test-1-pos-1-atmp-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 959/959 [00:00<00:00, 26387.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (723, 302) to (736, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "9it [00:00, 38.07it/s][swscaler @ 0x5612200] Warning: data is not aligned! This can lead to a speed loss\n",
      "959it [00:19, 48.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dust-test-1-pos-2-atmp-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [00:00<00:00, 26160.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (723, 302) to (736, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "16it [00:00, 42.77it/s][swscaler @ 0x56b8200] Warning: data is not aligned! This can lead to a speed loss\n",
      "577it [00:12, 47.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dust-test-1-pos-3-atmp-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 544/544 [00:00<00:00, 24877.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (723, 302) to (736, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "11it [00:00, 38.69it/s][swscaler @ 0x6d62200] Warning: data is not aligned! This can lead to a speed loss\n",
      "544it [00:11, 46.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dust-test-1-pos-3-atmp-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 663/663 [00:53<00:00, 12.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (723, 302) to (736, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "15it [00:00, 41.58it/s][swscaler @ 0x555d200] Warning: data is not aligned! This can lead to a speed loss\n",
      "663it [00:14, 45.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dust-test-1-pos-4-atmp-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:22<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (723, 302) to (736, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "13it [00:00, 35.48it/s][swscaler @ 0x5656200] Warning: data is not aligned! This can lead to a speed loss\n",
      "90it [00:02, 40.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dust-test-1-pos-5-atmp-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:28<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (723, 302) to (736, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "15it [00:00, 38.65it/s][swscaler @ 0x6717200] Warning: data is not aligned! This can lead to a speed loss\n",
      "1000it [00:23, 42.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dust-test-1-pos-5-atmp-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [05:22<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (723, 302) to (736, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "11it [00:00, 32.22it/s][swscaler @ 0x6616200] Warning: data is not aligned! This can lead to a speed loss\n",
      "1228it [00:29, 41.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in valid_notes:\n",
    "    print(key)\n",
    "    folder_name = Path(data_path) / 'humans_in_dust' / key\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    base_df = df_dusty[df_dusty['special_notes'] == key]\n",
    "    for id, df_row in tqdm(base_df.iterrows(), total=len(base_df)):\n",
    "        file_name = folder_name / str(id + '.png')\n",
    "        if not os.path.exists(file_name):\n",
    "            client.download_file(df_row['s3_bucket'], df_row['s3_key'], file_name)\n",
    "    create_video_frames(key, base_df=base_df, folder_name=f'humans_in_dust/{key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'front-center-left', 'front-left-left', 'front-right-left'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_dusty['camera_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_human_images = {\n",
    "    'Dust-test-1-pos-1': { # very light dust\n",
    "        'front-left-left': [('01:12:27', '01:13:26')],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': []\n",
    "    },\n",
    "    'Dust-test-1-pos-1-atmp-2': { # medium dust\n",
    "        'front-left-left': [('01:18:42', '01:20:20')],\n",
    "        'front-center-left': [],\n",
    "        'front-right-left': []\n",
    "    },\n",
    "    'Dust-test-1-pos-2-atmp-1': { # medium dust\n",
    "        'front-left-left': [('01:20:28', '01:20:36')],\n",
    "        'front-center-left': [('01:22:46', '01:23:28')],\n",
    "        'front-right-left': []\n",
    "    },\n",
    "    'Dust-test-1-pos-3-atmp-1': { # light dust\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('01:24:15', '01:25:20')],\n",
    "        'front-right-left': []\n",
    "    },\n",
    "    'Dust-test-1-pos-3-atmp-2': { # heavy dust\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('01:34:19', '01:35:15')],\n",
    "        'front-right-left': []\n",
    "    },\n",
    "    'Dust-test-1-pos-4-atmp-1': { # medium dust\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('01:43:03', '1:43:10')],\n",
    "        'front-right-left': []\n",
    "    },\n",
    "    'Dust-test-1-pos-5-atmp-1': { # heavy dust\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('01:37:07', '01:39:05')],\n",
    "        'front-right-left': []\n",
    "    },\n",
    "    'Dust-test-1-pos-5-atmp-2': { # heavy dust\n",
    "        'front-left-left': [],\n",
    "        'front-center-left': [('01:39:07', '01:43:00')],\n",
    "        'front-right-left': []\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_human_runs = []\n",
    "for notes in valid_notes:\n",
    "    for camera_location in ['front-left-left', 'front-center-left', 'front-right-left']:\n",
    "        times = only_human_images[notes][camera_location]\n",
    "        for start_t, end_t in times:\n",
    "            sequence = df_dusty.loc[(df_dusty['special_notes'] == notes) & (df_dusty['camera_location'] == camera_location)]\n",
    "            filtered = filter_movie(sequence, start_t, end_t)\n",
    "            if not len(filtered):\n",
    "                print(run_id, start_t, end_t) # oops\n",
    "            cleaned_human_runs.append(filtered)\n",
    "human_dusty_df = pd.concat(cleaned_human_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1650"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(human_dusty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.create(\n",
    "    name='mannequin_in_dust',\n",
    "    description=\"8 sequences of a mannequin in front of the tractor with dust blowing into it. All images contain a mannequin. Collected 2023 July 7. Left cameras only (1650 images)\",\n",
    "    kind=Dataset.KIND_IMAGE,\n",
    "    image_ids=list(human_dusty_df['image_id']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
