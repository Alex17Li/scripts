{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from brtdevkit.core.db.athena import AthenaClient\n",
    "from brtdevkit.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "europa_path = '/home/alexli/JupiterCVML-master/europa/base/src/europa'\n",
    "if europa_path not in sys.path:\n",
    "    sys.path.append(europa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa04df6",
   "metadata": {},
   "source": [
    "## Set up important constants and info\n",
    "\n",
    "For example, find the previous training data and make sure it is excluded. We specifically need to look at previous HALO data, but we should also look at previous core train data as well long term.\n",
    "\n",
    "For now, we don't do this because the query takes a very long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b443334",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_TRAIN_DATASETS = [\n",
    "    \"halo_rgb_stereo_train_v6_2\"\n",
    "]\n",
    "KNOWN_PRODUCTIVITY_DATASETS = [\n",
    "    # DAY\n",
    "    \"20230929_halo_rgb_productivity_day_candidate_10_dirty\",\n",
    "    \"20230929_halo_rgb_productivity_day_candidate_8_dirty\"\n",
    "    \"20230929_halo_rgb_productivity_day_candidate_14_dirty\",\n",
    "    \"20230929_halo_rgb_productivity_day_candidate_13_dirty\"\n",
    "    \"20230929_halo_rgb_productivity_day_candidate_6_dirty\",\n",
    "    \"20230929_halo_rgb_productivity_day_candidate_4_dirty\",\n",
    "    # \"20230925_halo_rgb_productivity_day_candidate_3\",\n",
    "    # \"20230925_halo_rgb_productivity_day_candidate_2\",\n",
    "    \"20230912_halo_rgb_productivity_day_candidate_1\",\n",
    "    \"20230912_halo_rgb_productivity_day_candidate_0\",\n",
    "    # NIGHT\n",
    "    \"20230929_halo_rgb_productivity_night_candidate_4_dirty\",\n",
    "    \"20230929_halo_rgb_productivity_night_candidate_3_dirty\",\n",
    "    \"20230912_halo_rgb_productivity_night_candidate_0\",\n",
    "    # \"20230925_halo_rgb_productivity_night_candidate_1\",\n",
    "    # \"20230925_halo_rgb_productivity_night_candidate_2\",\n",
    "]\n",
    "GEOHASH_SHORT = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the existing train datasets' short geohashes to exclude\n",
    "dfs = []\n",
    "for dataset_name in KNOWN_TRAIN_DATASETS:\n",
    "    dfs.append(Dataset.retrieve(name=dataset_name).to_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the existing productivity datasets' short geohashes to exclude (optional)\n",
    "for dataset_name in KNOWN_PRODUCTIVITY_DATASETS:\n",
    "    dfs.append(Dataset.retrieve(name=dataset_name).to_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"geohash_short\"] = train_df[\"geohash\"].apply(lambda x: x[:GEOHASH_SHORT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_GEOHASHES = list(set(train_df[\"geohash_short\"]))\n",
    "print(len(EXCLUDE_GEOHASHES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"robot_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b547d",
   "metadata": {},
   "source": [
    "## Construct the Athena query for the candidate data\n",
    "\n",
    "There are a few things that we are looking for in this Athena query.\n",
    "\n",
    "1. Cannot have overlapping geohashes with the `EXCLUDE_GEOHASHES` string above.\n",
    "2. Must have a valid geohash. \n",
    "3. Must have images from all cams present. This requires a few different checks\n",
    "   described in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93b5df",
   "metadata": {},
   "source": [
    "### Querying for images from all cams\n",
    "\n",
    "Querying for sets where all cams are available is somewhat tricky with Halo. There are\n",
    "a few criteria that need to be satisfied.\n",
    "\n",
    "1. For a given group ID, if it is a tractor pod it must have 8 images present. If it is\n",
    "   an implement pod, it must have 4 images present.\n",
    "2. For each group ID, there must be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b17ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "athena = AthenaClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5228b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_cameras = ('T01', 'T02', 'T05', 'T06', 'T09', 'T10', 'T13', 'T14', 'I01', 'I02')\n",
    "IMPLEMENT_CAMS = ('I01', 'I02', 'I03', 'I04')\n",
    "FRONT_POD_CAMS = ('T01', 'T02', 'T03', 'T04', 'T05', 'T06', 'T07', 'T08')\n",
    "REAR_POD_CAMS = ('T09', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT id, geohash, group_id, collected_on, robot_name, bag_name, camera_location,\n",
    "        operation_time, gps_can_data__json, special_notes, soil_color, operating_field_name,\n",
    "        implement, state, weather, field_conditions, terrain_type, farm, hard_drive_name\n",
    "    FROM image_jupiter\n",
    "    WHERE sensor_type = 'VD6763'\n",
    "    AND robot_name NOT LIKE 'bedrock%'\n",
    "    AND gps_can_data__json IS NOT NULL\n",
    "    AND geohash NOT LIKE '7zzzz%'\n",
    "    AND SUBSTRING(geohash, 1, {GEOHASH_SHORT}) NOT IN {tuple(EXCLUDE_GEOHASHES)}\n",
    "    ORDER BY \"collected_on\"\n",
    "\"\"\"\n",
    "# AND collected_on BETWEEN TIMESTAMP '2023-08-01 00:00:00' AND TIMESTAMP '2024-08-03 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "df = athena.get_df(query)\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e253381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"geohash_short\"] = df[\"geohash\"].apply(lambda x: x[:GEOHASH_SHORT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec901f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gps_can_data'] = df[\"gps_can_data__json\"].apply(lambda x: json.loads(x))\n",
    "df['speed'] = df['gps_can_data'].apply(lambda x: x.get('speed', np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5feaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"speed\"] > 0.1] # good to actually get moving sequences, but might be too restrictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"speed\"] < 30] # try to remove images where we're just on a road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a46005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional visualization of stats\n",
    "print(df[\"soil_color\"].value_counts(), end=\"\\n\\n\")\n",
    "print(df[\"weather\"].value_counts(), end=\"\\n\\n\")\n",
    "print(df[\"field_conditions\"].value_counts(), end=\"\\n\\n\")\n",
    "print(df[\"operation_time\"].value_counts(), end=\"\\n\\n\")\n",
    "print(df[\"terrain_type\"].value_counts(), end=\"\\n\\n\")\n",
    "print(df[\"implement\"].value_counts(), end=\"\\n\\n\")\n",
    "print(df[\"state\"].value_counts(), end=\"\\n\\n\")\n",
    "print(df[\"state\"].isna().value_counts(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all images without full pods\n",
    "# counter essentially checks whether unique elements of a list are a permutation of each other\n",
    "def is_full_pod(list1):\n",
    "    l1_counter = Counter(list1)\n",
    "    return (\n",
    "        l1_counter == Counter(IMPLEMENT_CAMS)\n",
    "        or l1_counter == Counter(FRONT_POD_CAMS)\n",
    "        or l1_counter == Counter(REAR_POD_CAMS)\n",
    "    )\n",
    "\n",
    "df_only_valid = df.groupby(\"group_id\").filter(\n",
    "    lambda x: is_full_pod(x[\"camera_location\"].tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47454f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_only_valid))\n",
    "print(df_only_valid[\"state\"].isna().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94fe643",
   "metadata": {},
   "source": [
    "## Find full sequences\n",
    "\n",
    "The next step is to find sequences of images with lengths of something like 5 minutes.\n",
    "These need **completely full** pods, so all three pods need to be full, and each group\n",
    "of images should be within 1 second of each other from the pods.\n",
    "\n",
    "(The exact details of this may change - for example loosening the number of seconds between images\n",
    "or whether the implement pod is required.)\n",
    "\n",
    "The idea here is to use a graph building algorithm similar to DBSCAN, where the distance\n",
    "between nodes is based on the `collected_on` timestamp. Images belonging to the same\n",
    "cluster can be considered part of a sequence. Checks are made to ensure that the correct\n",
    "machine is used when grouping images together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_graph(df: pd.DataFrame, seconds_apart: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Basically runs a DBSCAN algorithm, using the collected on as the metric for which we\n",
    "    are finding distance, and using an epsilon in seconds.\n",
    "\n",
    "    Ideally, this should find connected sequences of images. This should be run on dataframes\n",
    "    for each machine - which is a combination of three robot names (one for each pod).\n",
    "    \"\"\"\n",
    "    # convert collected-on timestamp to to posix float\n",
    "    df[\"posix_timestamp\"] = df[\"collected_on\"].apply(lambda x: pd.Timestamp(x).timestamp())\n",
    "    \n",
    "    # convert the posix timestamp to numpy array\n",
    "    timestamps = df[\"posix_timestamp\"].to_numpy().reshape((-1, 1))\n",
    "    try:\n",
    "        dbscan = DBSCAN(eps=seconds_apart).fit(X=timestamps)\n",
    "        df[\"sequence_id\"] = dbscan.labels_.astype(int)\n",
    "    except:\n",
    "        print(\"Warning: empty dataframe.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the valid dataframe by machine\n",
    "df_only_valid[\"robot_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a555192",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH0 = [\"halohitchhiker_101\", \"halohitchhiker_102\", \"halohitchhiker_103\"]\n",
    "HH1 = [\"halohitchhiker_111\", \"halohitchhiker_112\", \"halohitchhiker_113\"]\n",
    "HH2 = [\"halohitchhiker_121\", \"halohitchhiker_122\", \"halohitchhiker_123\"]\n",
    "HH6 = [\"halohitchhiker_161\", \"halohitchhiker_162\", \"halohitchhiker_163\"]\n",
    "HH20 = [\"halohitchhiker_201\", \"halohitchhiker_202\", \"halohitchhiker_203\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hh0 = df_only_valid[df_only_valid[\"robot_name\"].isin(HH0)].copy()\n",
    "df_hh1 = df_only_valid[df_only_valid[\"robot_name\"].isin(HH1)].copy()\n",
    "df_hh2 = df_only_valid[df_only_valid[\"robot_name\"].isin(HH2)].copy()\n",
    "# df_hh6 = df_only_valid[df_only_valid[\"robot_name\"].isin(HH6)].copy()\n",
    "# df_hh20 = df_only_valid[df_only_valid[\"robot_name\"].isin(HH20)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ce4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hh0 = sequence_graph(df_hh0, seconds_apart=20)\n",
    "df_hh1 = sequence_graph(df_hh1, seconds_apart=20)\n",
    "df_hh2 = sequence_graph(df_hh2, seconds_apart=20)\n",
    "# df_hh6 = sequence_graph(df_hh6, seconds_apart=20)\n",
    "# df_hh20 = sequence_graph(df_hh20, seconds_apart=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_hh0[\"sequence_id\"].unique()))\n",
    "print(len(df_hh1[\"sequence_id\"].unique()))\n",
    "print(len(df_hh2[\"sequence_id\"].unique()))\n",
    "# print(len(df_hh6[\"sequence_id\"].unique()))\n",
    "# print(len(df_hh20[\"sequence_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_bots = pd.concat([df_hh0, df_hh1, df_hh2])\n",
    "df_all_bots[\"sequence_id\"] = df_all_bots.apply(lambda x: str(int(x[\"sequence_id\"])) + f\"_hh{x['robot_name'][-3:-1]}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f222639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can focus on more metadata depending on our needs\n",
    "# a few examples are provided and commented out\n",
    "# operation time\n",
    "# df_all_bots = df_all_bots[df_all_bots[\"operation_time\"] == \"nightime\"]\n",
    "\n",
    "# soil color\n",
    "# df_all_bots = df_all_bots[df_all_bots[\"soil_color\"] == \"loamy\"]\n",
    "\n",
    "# field conditions (e.g. crop type)\n",
    "# df_all_bots = df_all_bots[(df_all_bots[\"field_conditions\"] != \"corn\") | df_all_bots[\"field_conditions\"].isna()]\n",
    "\n",
    "# weather conditions\n",
    "# df_all_bots = df_all_bots[df_all_bots[\"weather\"] == \"sunny\"]\n",
    "\n",
    "# terrain type\n",
    "df_all_bots = df_all_bots[(df_all_bots[\"terrain_type\"] != \"headlands\") | df_all_bots[\"terrain_type\"].isna()]\n",
    "\n",
    "# fields\n",
    "EXCLUDE_FIELDS = [ # generally fields we already have represented\n",
    "    # \"Field 7\",\n",
    "    \"0929-Leka Homeplace\",\n",
    "    \"1101- Island  S\",\n",
    "    \"1102-Schien W\",\n",
    "    \"1100- Island  N\",\n",
    "]\n",
    "df_all_bots[\"operating_field_name\"].fillna(\"unknown\", inplace=True)\n",
    "df_all_bots = df_all_bots[~(df_all_bots[\"operating_field_name\"].isin(EXCLUDE_FIELDS))]\n",
    "# df_all_bots = df_all_bots[df_all_bots[\"operating_field_name\"] == \"1106- Mitchel\"]\n",
    "\n",
    "# geohash\n",
    "df_all_bots = df_all_bots[~df_all_bots[\"geohash\"].str.startswith(\"9xj\") & ~df_all_bots[\"geohash\"].str.startswith(\"dp0\")]\n",
    "\n",
    "# implement\n",
    "# df_all_bots = df_all_bots[df_all_bots[\"implement\"] == \"BR96\"]\n",
    "\n",
    "# state\n",
    "df_all_bots[\"state\"].fillna(\"unknown\", inplace=True)\n",
    "df_all_bots = df_all_bots[(~(df_all_bots[\"state\"] == \"Illinois\") & ~(df_all_bots[\"state\"] == \"Colorado\"))]\n",
    "print(df_all_bots[\"state\"].value_counts())\n",
    "\n",
    "print(len(df_all_bots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d628d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out bad sequences\n",
    "# these might be sequences with missing VPUs\n",
    "# or sequences that are too short\n",
    "def check_sequence(x) -> bool:\n",
    "    # check bad vpu\n",
    "    if len(x[\"camera_location\"].value_counts()) < 16: # allow sequences without implement cams\n",
    "        return False\n",
    "    # check too short\n",
    "    if x[\"posix_timestamp\"].max() - x[\"posix_timestamp\"].min() < 300: # 5 min, can change this\n",
    "        return False\n",
    "    # if x[\"posix_timestamp\"].max() - x[\"posix_timestamp\"].min() < 120: # 2 min, can change this\n",
    "    #     return False\n",
    "    # check bad speed (too slow or stationary)\n",
    "    if x[\"speed\"].max() < 1:\n",
    "        return False\n",
    "    # check big discepancy between camera location value counts\n",
    "    # cam_value_counts = x[\"camera_location\"].value_counts()\n",
    "    # if (cam_value_counts.max() - cam_value_counts.min()) * 1.0 / (cam_value_counts.max()) > 0.1:\n",
    "    #     return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_all_bots.groupby(\"sequence_id\").filter(check_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)\n",
    "df_tmp.groupby([\"sequence_id\", \"state\"])[\"operation_time\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec92cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tmp[\"sequence_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab only the images of a specific sequence (see output of above cell for sequence names)\n",
    "df_tmp = df_all_bots[df_all_bots[\"sequence_id\"] == \"5_hh10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a98dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tmp[\"posix_timestamp\"].max() - df_tmp[\"posix_timestamp\"].min())\n",
    "# print(df_tmp[\"posix_timestamp\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell just visualizes some stats - not required\n",
    "print(df_tmp[\"camera_location\"].value_counts())\n",
    "# print(df_tmp[\"robot_name\"].value_counts())\n",
    "print(df_tmp[\"field_conditions\"].value_counts())\n",
    "print(df_tmp[\"farm\"].value_counts())\n",
    "print(df_tmp[\"operating_field_name\"].value_counts())\n",
    "print(df_tmp[\"collected_on\"].min())\n",
    "print(df_tmp[\"collected_on\"].max())\n",
    "print(df_tmp[\"speed\"].max())\n",
    "print(df_tmp[\"state\"].value_counts())\n",
    "print(df_tmp[\"operation_time\"].value_counts())\n",
    "print(df_tmp[\"weather\"].value_counts())\n",
    "print(df_tmp[(df_tmp[\"camera_location\"] == \"T12\")][\"id\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes we only want specific bags of a sequence, since the full sequence may have some issues\n",
    "# df_tmp = df_tmp[(df_tmp[\"bag_name\"] == \"07_20_2023-16_45_47\") | (df_tmp[\"bag_name\"] == \"07_20_2023-16_45_45\")]\n",
    "# print(len(df_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset candidates\n",
    "image_ids = list(set(df_tmp[\"id\"].tolist()))\n",
    "print(len(image_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d030a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.create(\n",
    "    name=\"20230929_halo_rgb_productivity_day_candidate_14_dirty\",\n",
    "    description=\"A day sequence, roughly 40 minutes long, ~1765 images per camera. Ground has low corn residue, loamy soil, some objects on horizon far away.\",\n",
    "    kind=Dataset.KIND_IMAGE,\n",
    "    image_ids=image_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.create(\n",
    "    name=\"20230929_halo_rgb_productivity_night_candidate_4_dirty\",\n",
    "    description=\"A night sequence, roughly 141 minutes long, ~4600 images per camera. Long sequence in what looks like pretty much a dirt field. Starts a bit after sundown (no sun in frame) to night. Some images especially around implement at night have minimal features other than dust.\",\n",
    "    kind=Dataset.KIND_IMAGE,\n",
    "    image_ids=image_ids,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
